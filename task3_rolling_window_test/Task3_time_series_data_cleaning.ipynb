{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from classification_models import GetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>trade_date</th>\n",
       "      <th>factor_0</th>\n",
       "      <th>factor_1</th>\n",
       "      <th>factor_2</th>\n",
       "      <th>factor_3</th>\n",
       "      <th>factor_4</th>\n",
       "      <th>factor_5</th>\n",
       "      <th>factor_6</th>\n",
       "      <th>factor_7</th>\n",
       "      <th>...</th>\n",
       "      <th>factor_33</th>\n",
       "      <th>factor_34</th>\n",
       "      <th>factor_35</th>\n",
       "      <th>factor_36</th>\n",
       "      <th>factor_37</th>\n",
       "      <th>factor_38</th>\n",
       "      <th>factor_39</th>\n",
       "      <th>factor_40</th>\n",
       "      <th>factor_41</th>\n",
       "      <th>car_hs300_b30_d001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.032475</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-0.114170</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.440203</td>\n",
       "      <td>-0.376429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.724551</td>\n",
       "      <td>0.812212</td>\n",
       "      <td>20.869800</td>\n",
       "      <td>43.5275</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>0.680235</td>\n",
       "      <td>1.57469</td>\n",
       "      <td>3.43924</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>0.023154</td>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.589069</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.129240</td>\n",
       "      <td>-0.840631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856688</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.841909</td>\n",
       "      <td>18.507200</td>\n",
       "      <td>41.6409</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>0.704779</td>\n",
       "      <td>1.47261</td>\n",
       "      <td>3.15030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.026994</td>\n",
       "      <td>0.031288</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-0.150607</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.957488</td>\n",
       "      <td>-0.160806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676724</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.786972</td>\n",
       "      <td>20.082300</td>\n",
       "      <td>29.5337</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.705782</td>\n",
       "      <td>1.48008</td>\n",
       "      <td>3.02077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.013784</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-0.836842</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.802262</td>\n",
       "      <td>-0.532596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.759596</td>\n",
       "      <td>8.662940</td>\n",
       "      <td>27.9438</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.699468</td>\n",
       "      <td>1.47922</td>\n",
       "      <td>3.01475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.019546</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.565992</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.452223</td>\n",
       "      <td>-0.712085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.628788</td>\n",
       "      <td>12.206900</td>\n",
       "      <td>27.2890</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.690864</td>\n",
       "      <td>1.51058</td>\n",
       "      <td>3.06918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088415</th>\n",
       "      <td>603999.SH</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-0.875646</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.138746</td>\n",
       "      <td>-0.876772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>1.148900</td>\n",
       "      <td>20.9302</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.307711</td>\n",
       "      <td>1.14262</td>\n",
       "      <td>3.09775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088416</th>\n",
       "      <td>603999.SH</td>\n",
       "      <td>2019-12-16</td>\n",
       "      <td>0.038682</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>0.045845</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-0.880784</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.306574</td>\n",
       "      <td>-0.751634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.811730</td>\n",
       "      <td>0.782227</td>\n",
       "      <td>27.0718</td>\n",
       "      <td>0.187879</td>\n",
       "      <td>0.276938</td>\n",
       "      <td>1.04110</td>\n",
       "      <td>2.77682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088417</th>\n",
       "      <td>603999.SH</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.016925</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-0.854110</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.863933</td>\n",
       "      <td>-0.479172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654971</td>\n",
       "      <td>0.778761</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.342224</td>\n",
       "      <td>29.0801</td>\n",
       "      <td>0.298013</td>\n",
       "      <td>0.260858</td>\n",
       "      <td>1.04584</td>\n",
       "      <td>2.66659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088418</th>\n",
       "      <td>603999.SH</td>\n",
       "      <td>2019-12-18</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.733061</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.511944</td>\n",
       "      <td>-0.716503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.736364</td>\n",
       "      <td>0.799807</td>\n",
       "      <td>0.391114</td>\n",
       "      <td>29.2537</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.252109</td>\n",
       "      <td>1.07986</td>\n",
       "      <td>2.67018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5088419</th>\n",
       "      <td>603999.SH</td>\n",
       "      <td>2019-12-19</td>\n",
       "      <td>0.100575</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.109195</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.902884</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>-0.962701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.838375</td>\n",
       "      <td>1.857790</td>\n",
       "      <td>40.8740</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.265855</td>\n",
       "      <td>1.21668</td>\n",
       "      <td>2.20963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5088420 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stock_code  trade_date  factor_0  factor_1  factor_2  factor_3  \\\n",
       "0        000001.SZ  2013-01-04  0.007966  0.024510  0.032475      -8.0   \n",
       "1        000001.SZ  2013-01-07  0.023154  0.006258  0.029412      -7.0   \n",
       "2        000001.SZ  2013-01-08  0.004295  0.026994  0.031288      -5.0   \n",
       "3        000001.SZ  2013-01-09  0.003759  0.010025  0.013784      -3.0   \n",
       "4        000001.SZ  2013-01-10  0.015132  0.004414  0.019546      -1.0   \n",
       "...            ...         ...       ...       ...       ...       ...   \n",
       "5088415  603999.SH  2019-12-13  0.063063  0.007508  0.070571      -9.0   \n",
       "5088416  603999.SH  2019-12-16  0.038682  0.007163  0.045845      -9.0   \n",
       "5088417  603999.SH  2019-12-17  0.002821  0.016925  0.019746      -8.0   \n",
       "5088418  603999.SH  2019-12-18  0.010029  0.012894  0.022923      -7.0   \n",
       "5088419  603999.SH  2019-12-19  0.100575  0.008621  0.109195      -7.0   \n",
       "\n",
       "         factor_4  factor_5  factor_6  factor_7  ...  factor_33  factor_34  \\\n",
       "0       -0.114170      0.03  0.440203 -0.376429  ...   0.790698   0.724551   \n",
       "1       -0.589069     -0.31  0.129240 -0.840631  ...   0.856688   0.909091   \n",
       "2       -0.150607      0.30  0.957488 -0.160806  ...   0.676724   0.727273   \n",
       "3       -0.836842      0.14  0.802262 -0.532596  ...   0.632231   0.642424   \n",
       "4       -0.565992     -0.01  0.452223 -0.712085  ...   0.629167   0.516667   \n",
       "...           ...       ...       ...       ...  ...        ...        ...   \n",
       "5088415 -0.875646     -0.32  0.138746 -0.876772  ...   0.542857   0.855769   \n",
       "5088416 -0.880784     -0.18  0.306574 -0.751634  ...   0.583333   0.884298   \n",
       "5088417 -0.854110      0.11  0.863933 -0.479172  ...   0.654971   0.778761   \n",
       "5088418 -0.733061      0.04  0.511944 -0.716503  ...   0.670659   0.736364   \n",
       "5088419 -0.902884     -0.70  0.019272 -0.962701  ...   0.777778   1.000000   \n",
       "\n",
       "         factor_35  factor_36  factor_37  factor_38  factor_39  factor_40  \\\n",
       "0         0.812212  20.869800    43.5275   0.741100   0.680235    1.57469   \n",
       "1         0.841909  18.507200    41.6409   0.741100   0.704779    1.47261   \n",
       "2         0.786972  20.082300    29.5337   0.605634   0.705782    1.48008   \n",
       "3         0.759596   8.662940    27.9438   0.536232   0.699468    1.47922   \n",
       "4         0.628788  12.206900    27.2890   0.520000   0.690864    1.51058   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "5088415   0.822900   1.148900    20.9302   0.029940   0.307711    1.14262   \n",
       "5088416   0.811730   0.782227    27.0718   0.187879   0.276938    1.04110   \n",
       "5088417   0.839609   0.342224    29.0801   0.298013   0.260858    1.04584   \n",
       "5088418   0.799807   0.391114    29.2537   0.324324   0.252109    1.07986   \n",
       "5088419   0.838375   1.857790    40.8740   0.574257   0.265855    1.21668   \n",
       "\n",
       "         factor_41  car_hs300_b30_d001  \n",
       "0          3.43924                   1  \n",
       "1          3.15030                   0  \n",
       "2          3.02077                   0  \n",
       "3          3.01475                   0  \n",
       "4          3.06918                   1  \n",
       "...            ...                 ...  \n",
       "5088415    3.09775                   1  \n",
       "5088416    2.77682                   0  \n",
       "5088417    2.66659                   0  \n",
       "5088418    2.67018                   1  \n",
       "5088419    2.20963                   1  \n",
       "\n",
       "[5088420 rows x 45 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入数据\n",
    "factor_df=pd.read_csv(\"/data01/data_for_intern/factor_data_13_19.csv\")\n",
    "return_df=pd.read_csv(\"/data01/data_for_intern/return_data_13_19.csv\")\n",
    "return_label=return_df.drop(columns=['return_adj_d001','return_adj_d005','car_hs300_b30_d005'])\n",
    "return_label['car_hs300_b30_d001']=(return_label['car_hs300_b30_d001']>=0)*1\n",
    "#利用内连接方式合并数据\n",
    "merged_factor_return = factor_df.merge(return_label,on=[\"stock_code\",\"trade_date\"],how=\"inner\")\n",
    "merged_factor_return"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def waste_date(date,merged_factor_return):\n",
    "    waste=[]\n",
    "    for i in range(len(date)-2):\n",
    "        temp=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\")].drop(['car_hs300_b30_d001'],axis=1)\n",
    "        d1=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\") & (merged_factor_return[\"trade_date\"]==date[i])].index\n",
    "        if d1.size==0:\n",
    "            print(\"waste\",date[i])\n",
    "            waste.append(date[i])\n",
    "            continue\n",
    "        d1=int(np.array(d1))\n",
    "        d2=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\") & (merged_factor_return[\"trade_date\"]==date[i])].index\n",
    "        if d2.size==0:\n",
    "            print(\"waste\",date[i])\n",
    "            waste.append(date[i])\n",
    "            continue\n",
    "        d2=int(np.array(d2))\n",
    "        \n",
    "    return waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛选日期节点,以15个交易日为界\n",
    "date=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date.append(\"2019-01-21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while(i>-1):\n",
    "    now=datetime.datetime.strptime(date[i], \"%Y-%m-%d\")\n",
    "    if date[i]==\"2019-12-02\":\n",
    "        break\n",
    "    delta=datetime.timedelta(days=21)\n",
    "    n_days=now+delta\n",
    "    date.append(n_days.strftime('%Y-%m-%d'))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-01-21',\n",
       " '2019-02-11',\n",
       " '2019-03-04',\n",
       " '2019-03-25',\n",
       " '2019-04-15',\n",
       " '2019-05-06',\n",
       " '2019-05-27',\n",
       " '2019-06-17',\n",
       " '2019-07-08',\n",
       " '2019-07-29',\n",
       " '2019-08-19',\n",
       " '2019-09-09',\n",
       " '2019-09-30',\n",
       " '2019-10-21',\n",
       " '2019-11-11',\n",
       " '2019-12-02']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000001.SZ', '000002.SZ', '000004.SZ', ..., '603997.SH',\n",
       "       '603998.SH', '603999.SH'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_name=np.unique(merged_factor_return[\"stock_code\"])\n",
    "idx_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(merged_factor_return,date1,date2,date3,idx_name, gate):\n",
    "    #data cleaning\n",
    "    if gate == 0 :\n",
    "        train_data=np.zeros([1,10,42])\n",
    "        valid_data=np.zeros([1,10,42])\n",
    "    \n",
    "        df_data=merged_factor_return.drop(['car_hs300_b30_d001'],axis=1)\n",
    "    \n",
    "        #training data\n",
    "        #waste=[]\n",
    "        for name in idx_name:\n",
    "            temp=df_data[(df_data[\"stock_code\"]==name)]\n",
    "            d1=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date1)].index\n",
    "            if d1.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date2)].index\n",
    "            if d2.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                train_temp=temp.loc[i:i+9].values.reshape(1,10,42)\n",
    "                train_data=np.concatenate((train_data,train_temp),axis=0)\n",
    "            \n",
    "        #waste2=[]\n",
    "        for name in idx_name:\n",
    "            temp=df_data[(df_data[\"stock_code\"]==name)]\n",
    "            d1=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste2.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                #waste2.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                valid_temp=temp.loc[i:i+9].values.reshape(1,10,42)\n",
    "                valid_data=np.concatenate((valid_data,valid_temp),axis=0)\n",
    "    \n",
    "        #label cleaning\n",
    "        label=merged_factor_return[[\"car_hs300_b30_d001\",\"stock_code\",\"trade_date\"]]\n",
    "\n",
    "        train_label=np.zeros([1,])\n",
    "        valid_label=np.zeros([1,])\n",
    "    \n",
    "        #waste=[]\n",
    "        for name in idx_name:\n",
    "            temp=label[(label[\"stock_code\"]==name)]\n",
    "            d1=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date1)].index\n",
    "            if d1.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date2)].index\n",
    "            if d2.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                train_temp=temp.loc[i+10].values\n",
    "                train_label=np.concatenate((train_label,train_temp),axis=0)\n",
    "    \n",
    "        #waste=[]\n",
    "        for name in idx_name:\n",
    "            temp=label[(label[\"stock_code\"]==name)]\n",
    "            d1=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                valid_temp=temp.loc[i+10].values\n",
    "                valid_label=np.concatenate((valid_label,valid_temp),axis=0)\n",
    "            \n",
    "        train_data=np.delete(train_data,0,axis=0)\n",
    "        valid_data=np.delete(valid_data,0,axis=0)\n",
    "        train_label=np.delete(train_label,0,axis=0)\n",
    "        valid_label=np.delete(valid_label,0,axis=0)\n",
    "        \n",
    "        np.save(\"data\"+str(gate),train_data)\n",
    "        np.save(\"label\"+str(gate),train_label)\n",
    "        np.save(\"data\"+str(gate+1),valid_data)        \n",
    "        np.save(\"label\"+str(gate+1),valid_label)    \n",
    "        \n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        train_data = np.load(\"data\"+str(gate)+\".npy\")\n",
    "        train_label = np.load(\"label\"+str(gate)+\".npy\")    \n",
    "\n",
    "        valid_data=np.zeros([1,10,42])\n",
    "        df_data=merged_factor_return.drop(['car_hs300_b30_d001'],axis=1)\n",
    "        for name in idx_name:\n",
    "            temp=df_data[(df_data[\"stock_code\"]==name)]\n",
    "            d1=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste2.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                #waste2.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                valid_temp=temp.loc[i:i+9].values.reshape(1,10,42)\n",
    "                valid_data=np.concatenate((valid_data,valid_temp),axis=0)\n",
    "\n",
    "        label=merged_factor_return[[\"car_hs300_b30_d001\",\"stock_code\",\"trade_date\"]]\n",
    "        valid_label=np.zeros([1,])\n",
    "\n",
    "        for name in idx_name:\n",
    "            temp=label[(label[\"stock_code\"]==name)]\n",
    "            d1=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2-9):\n",
    "                valid_temp=temp.loc[i+10].values\n",
    "                valid_label=np.concatenate((valid_label,valid_temp),axis=0)\n",
    "\n",
    "        valid_data=np.delete(valid_data,0,axis=0)\n",
    "        valid_label=np.delete(valid_label,0,axis=0)            \n",
    "        np.save(\"data\"+str(gate+1),valid_data)        \n",
    "        np.save(\"label\"+str(gate+1),valid_label)\n",
    "        \n",
    "    return train_data, valid_data, train_label, valid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成数据清洗的函数后开始定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,device=torch.device(\"cuda:1\")):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out\n",
    "    \n",
    "#搭建第一类ResNet block\n",
    "class BasicBlock(nn.Module):#基本残差网络的一个模块类\n",
    "    expansion = 1#每一个residual block中不改变width,height,channel数，即增加的residual部分不需要做卷积处理\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)#stride=1,kernel_size=3,padding=1保证了data的\n",
    "        self.drop = nn.Dropout(0.7)                                                                           #size不变 \n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:  #BasicBlock内不需要调整residual的height,width,channel\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "#dropout resnet+lstm\n",
    "#组建block成ResNet\n",
    "class ResNet1D_LSTM(nn.Module):\n",
    "    def __init__(self, block, blocks_num, num_classes, feature_channel,hidden_size, num_layers,device ):\n",
    "        super(ResNet1D_LSTM, self).__init__()\n",
    "        self.in_channel = 64\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_channel, out_channels=self.in_channel, kernel_size=2, stride=2,\n",
    "                               padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  #channel数变为n/2(非整数时向下取整)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])         #按照已有结论按二次方形式增长ResNet不同阶段的channel\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)#stride=2表示想把上一个layer传过来的size缩减为1/2\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        #self.avgpool = nn.AdaptiveAvgPool1d((1, 1))  # output size = (1, 1) \n",
    "        self.lstm = nn.LSTM(256, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.to(device), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #预处理\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x) #残差网络\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x=torch.transpose(x,2,1)\n",
    "        #因为pytorch里lstm和conv1d的input sequence位置不一样，需要调整。\n",
    "        \n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测\n",
    "def predict_precision(model,images,labels,device,predict_type):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        total=0\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=sum(predicted)\n",
    "        correct+=(sum(predicted*labels))\n",
    "        print('precision of the model on the'+predict_type+'data: {}%'.format(100*correct/total))\n",
    "    model.train()\n",
    "    return predicted, 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "def training(model, num_epochs, train_loader,valid_loader, loss_function, optimizer, device):\n",
    "    total_step=0\n",
    "    train_precision=[]\n",
    "    valid_precision=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            images=images.type(torch.FloatTensor)\n",
    "            labels=labels.type(torch.FloatTensor)\n",
    "            images=images.to(device)\n",
    "            labels=labels.to(device)\n",
    "            #forward pass\n",
    "            outputs=model(images)\n",
    "            loss=loss_function(outputs,labels.long())\n",
    "        \n",
    "            #Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_step+=1\n",
    "            if (total_step)%100==0:\n",
    "                #print(\"Epoch [{}/{}],step[{}] Loss:{:.4f}\".format(epoch+1,num_epochs,total_step,loss.item()))\n",
    "                _,train_pre=predict_precision(model,images,labels,device,predict_type='training')\n",
    "                train_precision.append(train_pre)\n",
    "                for images, labels in valid_loader:\n",
    "                    _,valid_pre=predict_precision(model,images,labels,device,predict_type='validation')\n",
    "                valid_precision.append(valid_pre)\n",
    "    \n",
    "    return train_precision, valid_precision, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_dele(data, label):\n",
    "    a=data\n",
    "    b=a.reshape([a.shape[0],a.shape[1]*a.shape[2]])\n",
    "    c=b[~np.isnan(b).any(axis=1),:]\n",
    "    data=c.reshape([c.shape[0], a.shape[1],a.shape[2]])\n",
    "    d=label.reshape(-1,1)\n",
    "    label=d[~np.isnan(b).any(axis=1),:].reshape(-1,)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, merged_factor_return, date, num_epochs, loss_function, optimizer, batch_size, device, data_reverse):\n",
    "    idx_name=np.unique(merged_factor_return[\"stock_code\"])\n",
    "    sum_train_precision=[]\n",
    "    sum_valid_precision=[]\n",
    "    for i in range( len(date)-2):\n",
    "        print(i)\n",
    "        date1=date[i]\n",
    "        date2=date[i+1]\n",
    "        date3=date[i+2]\n",
    "        train_data, valid_data, train_label, valid_label = data_cleaning(merged_factor_return,date1,date2,date3,idx_name,i)\n",
    "        train_data, train_label = nan_dele(train_data, train_label)\n",
    "        valid_data, valid_label = nan_dele(valid_data, valid_label)\n",
    "        if train_data.size==0:\n",
    "            print(\"waste\",date[i])\n",
    "            continue\n",
    "        if data_reverse == 1:\n",
    "            train_data=np.transpose(train_data,(0,2,1))\n",
    "            valid_data=np.transpose(valid_data,(0,2,1))\n",
    "        train=GetLoader(train_data,train_label)\n",
    "        valid=GetLoader(valid_data,valid_label)\n",
    "        train_loader=torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "        valid_loader=torch.utils.data.DataLoader(dataset=valid,batch_size=valid_data.shape[0],shuffle=False,num_workers=0)\n",
    "        train_precision, valid_precision, model = training(model, num_epochs, train_loader,valid_loader, loss_function, optimizer, device)\n",
    "        sum_train_precision.append(train_precision)\n",
    "        sum_valid_precision.append(valid_precision)\n",
    "        print(sum_train_precision)\n",
    "        np.save('sum_train_precision2',sum_train_precision)\n",
    "        np.save('sum_valid_precision2',sum_valid_precision)\n",
    "        \n",
    "    return sum_train_precision, sum_valid_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10  # 序列长度，将图像的每一列作为一个序列\n",
    "input_size = 42  # 输入数据的维度\n",
    "hidden_size = 64  # 隐藏层的size\n",
    "num_layers =  4 # 有多少层\n",
    "\n",
    "num_classes = 2\n",
    "batch_size = 256\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes,device=device)\n",
    "lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum_train_precision, sum_valid_precision = model_test(lstm, merged_factor_return, date, num_epochs, loss_function, optimizer, batch_size, device, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#整理datacleaning函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,15):\n",
    "    temp=np.load(\"label\"+str(i)+\".npy\")\n",
    "    print(i,\":\",temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_data_cleaning(merged_factor_return,date,idx_name, gate):\n",
    "    #data cleaning\n",
    "    date2, date3 = date\n",
    "    if date3!='2019-10-07':\n",
    "\n",
    "        print(gate)   \n",
    "        valid_data=np.zeros([1,10,42])\n",
    "    \n",
    "        df_data=merged_factor_return.drop(['car_hs300_b30_d001'],axis=1)\n",
    "    \n",
    "     \n",
    "        #waste2=[]\n",
    "        for name in idx_name:\n",
    "            #print(name)\n",
    "            temp=df_data[(df_data[\"stock_code\"]==name)]           \n",
    "            d1=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste2.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            #print('d1')\n",
    "            d2=df_data[(df_data[\"stock_code\"]==name) & (df_data[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                 #waste2.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            #print(range(d1,d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2):\n",
    "                valid_temp=temp.loc[i:i+9].values.reshape(1,10,42)\n",
    "                #print(valid_temp)\n",
    "                valid_data=np.concatenate((valid_data,valid_temp),axis=0)\n",
    "            #print(valid_data.shape)\n",
    "\n",
    "        valid_data=np.delete(valid_data,0,axis=0)\n",
    "        np.save(\"new_valid_data\"+str(gate),valid_data)\n",
    "    \n",
    "        label=merged_factor_return[[\"car_hs300_b30_d001\",\"stock_code\",\"trade_date\"]]\n",
    "        valid_label=np.zeros([1,])\n",
    "\n",
    "        #waste=[]\n",
    "        for name in idx_name:\n",
    "            temp=label[(label[\"stock_code\"]==name)]\n",
    "            d1=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date2)].index\n",
    "            if d1.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d1=int(np.array(d1))\n",
    "            d2=label[(label[\"stock_code\"]==name) & (label[\"trade_date\"]==date3)].index\n",
    "            if d2.size==0:\n",
    "                #waste.append(name)\n",
    "                continue\n",
    "            d2=int(np.array(d2))\n",
    "            temp=temp.drop([\"stock_code\",\"trade_date\"],axis=1)\n",
    "            for i in range(d1,d2):\n",
    "                valid_temp=temp.loc[i+10].values\n",
    "                valid_label=np.concatenate((valid_label,valid_temp),axis=0)\n",
    "\n",
    "\n",
    "        valid_label=np.delete(valid_label,0,axis=0)\n",
    "                \n",
    "        np.save(\"new_valid_label\"+str(gate),valid_label)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_date=[]\n",
    "i=2\n",
    "while(i<=len(date)):\n",
    "    now=datetime.datetime.strptime(date[i-1], \"%Y-%m-%d\")\n",
    "    delta=datetime.timedelta(days=2)\n",
    "    n_days=now+delta\n",
    "    valid_date.append((date[i-1],n_days.strftime('%Y-%m-%d')))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('2019-02-11', '2019-02-13'),\n",
       "  ('2019-03-04', '2019-03-06'),\n",
       "  ('2019-03-25', '2019-03-27'),\n",
       "  ('2019-04-15', '2019-04-17'),\n",
       "  ('2019-05-06', '2019-05-08'),\n",
       "  ('2019-05-27', '2019-05-29'),\n",
       "  ('2019-06-17', '2019-06-19'),\n",
       "  ('2019-07-08', '2019-07-10'),\n",
       "  ('2019-07-29', '2019-07-31'),\n",
       "  ('2019-08-19', '2019-08-21'),\n",
       "  ('2019-09-09', '2019-09-11'),\n",
       "  ('2019-09-30', '2019-10-02'),\n",
       "  ('2019-10-21', '2019-10-23'),\n",
       "  ('2019-11-11', '2019-11-13'),\n",
       "  ('2019-12-02', '2019-12-04')],\n",
       " 15,\n",
       " ('2019-02-11', '2019-02-13'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_date, len(valid_date), valid_date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(valid_date)):\n",
    "    valid_data_cleaning(merged_factor_return,valid_date[i],idx_name, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17920, 10, 42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('new_valid_data0.npy').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og=[]\n",
    "for i in range(len(valid_date)):\n",
    "    _,a=valid_date[i]\n",
    "    og.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waste_date(date,merged_factor_return):\n",
    "    waste=[]\n",
    "    for i in range(len(date)-2):\n",
    "        temp=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\")].drop(['car_hs300_b30_d001'],axis=1)\n",
    "        d1=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\") & (merged_factor_return[\"trade_date\"]==date[i])].index\n",
    "        if d1.size==0:\n",
    "            print(\"waste\",date[i])\n",
    "            waste.append(date[i])\n",
    "            continue\n",
    "        d1=int(np.array(d1))\n",
    "        d2=merged_factor_return[(merged_factor_return[\"stock_code\"]==\"000001.SZ\") & (merged_factor_return[\"trade_date\"]==date[i])].index\n",
    "        if d2.size==0:\n",
    "            print(\"waste\",date[i])\n",
    "            waste.append(date[i])\n",
    "            continue\n",
    "        d2=int(np.array(d2))\n",
    "        \n",
    "    return waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waste_date(og,merged_factor_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.random.rand(9,10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        m=np.delete(m,0,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
