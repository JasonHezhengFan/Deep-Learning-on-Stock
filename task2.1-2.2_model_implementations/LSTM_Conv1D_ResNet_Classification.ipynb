{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建RNN-LSTM Model (Many-to-One)\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#利用图像识别数据集验证LSTM是否成功\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "print(digits.images.shape)\n",
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "d2d = digits.images.reshape(1797,64,)\n",
    "df = pd.DataFrame(d2d)\n",
    "df['target'] = digits.target\n",
    "df.head()\n",
    "itrain, itest = train_test_split(range(df.shape[0]), train_size=0.6)\n",
    "set1 = {}\n",
    "set1['Xtrain'] = df[list(range(64))].iloc[itrain, :]\n",
    "set1['Xtest'] = df[list(range(64))].iloc[itest, :]\n",
    "set1['ytrain'] = df.target.iloc[itrain]\n",
    "set1['ytest'] = df.target.iloc[itest]\n",
    "x_train=np.array(set1['Xtrain'])\n",
    "x_train=x_train.reshape(1078,8, 8)\n",
    "x_test=np.array(set1['Xtest'])\n",
    "x_test=x_test.reshape(719,8, 8)\n",
    "y_train=np.array(set1['ytrain'])\n",
    "y_train=y_train.reshape(1078, )\n",
    "y_test=np.array(set1['ytest']) \n",
    "y_test=y_test.reshape(719,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "input_size = 8  # 输入数据的维度\n",
    "hidden_size = 256  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes,device=device)\n",
    "lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "#实现custom pytorch dataset\n",
    "class GetLoader(Dataset):\n",
    "# 初始化函数，得到数据\n",
    "    def __init__(self, data_root, data_label):\n",
    "        self.data = data_root\n",
    "        self.label = data_label\n",
    "    # index是根据batchsize划分数据后得到的索引，最后将data和对应的labels进行一起返回\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        labels = self.label[index]\n",
    "        return data, labels\n",
    "    # 该函数返回数据大小长度，目的是DataLoader方便划分，如果不知道大小，DataLoader会一执行错误\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=GetLoader(x_train,y_train)\n",
    "test=GetLoader(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True,num_workers=1)\n",
    "test_loader=torch.utils.data.DataLoader(dataset=test,batch_size=719,shuffle=False,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#训练过程\n",
    "total_step=0\n",
    "for epoch in range(300):\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        #forward pass\n",
    "        outputs=lstm(images)\n",
    "        loss=loss_function(outputs,labels.long())\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step+=1\n",
    "        if (total_step)%10==0:#each 10 iterations is one epoch\n",
    "            print(\"Epoch [{}/{}],step[{}] Loss:{:.4f}\".format(epoch+1,num_epochs,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测过程, 尝试了deeper的network和不同参数，基本上在mnist效果都差不多；\n",
    "#跟resnet相比效果也差不多；但LSTM训练速度明显快于ResNet且迭代效果也更好\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=lstm(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total=labels.size(0)\n",
    "        correct=(predicted==labels).sum().item()\n",
    "print('accuracy of the model on the test images: {}%'.format(100*correct/total))\n",
    "#accuracy of the model on the test images: 97.63560500695411%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建CONV1D-LSTM Model (Many-to-One)\n",
    "class CONV1D_LSTM(nn.Module):\n",
    "    def __init__(self ,in_channel,out_channel, hidden_size, num_layers, num_classes,device):\n",
    "        super(CONV1D_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channel, out_channels=out_channel,kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel,kernel_size=1, stride=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(out_channel, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight.to(device), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #forward prop\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out=torch.transpose(out,2,1)\n",
    "        #因为pytorch里lstm和conv1d的input sequence位置不一样，需要调整。\n",
    "        \n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, out.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, out.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(out, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "in_channel = 8\n",
    "out_channel=32\n",
    "hidden_size = 128  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1d_lstm = CONV1D_LSTM(in_channel=in_channel, out_channel=out_channel,\n",
    "                   hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes,device=device)\n",
    "conv1d_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(conv1d_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#训练过程\n",
    "total_step=0\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        #forward pass\n",
    "        outputs=conv1d_lstm(images)\n",
    "        loss=loss_function(outputs,labels.long())\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step+=1\n",
    "        if (total_step)%10==0:#each 10 iterations is one epoch\n",
    "            print(\"Epoch [{}/{}],step[{}] Loss:{:.4f}\".format(epoch+1,num_epochs,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测过程, 和普通lstm效果差不多\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=conv1d_lstm(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total=labels.size(0)\n",
    "        correct=(predicted==labels).sum().item()\n",
    "print('accuracy of the model on the test images: {}%'.format(100*correct/total))\n",
    "#accuracy of the model on the test images: 98.19193324061196%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建ResNet-LSTM Model (Many-to-One) 相较于简单的cnn-lstm，resnet可以防止梯度消失\n",
    "#搭建第一类ResNet block\n",
    "class BasicBlock(nn.Module):#基本残差网络的一个模块类\n",
    "    expansion = 1#每一个residual block中不改变width,height,channel数，即增加的residual部分不需要做卷积处理\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)#stride=1,kernel_size=3,padding=1保证了data的\n",
    "                                                                                   #size不变 \n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:  #BasicBlock内不需要调整residual的height,width,channel\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组建block成ResNet\n",
    "class ResNet1D_LSTM(nn.Module):\n",
    "    def __init__(self, block, blocks_num, num_classes, feature_channel,hidden_size, num_layers,device ):\n",
    "        super(ResNet1D_LSTM, self).__init__()\n",
    "        self.in_channel = 64\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_channel, out_channels=self.in_channel, kernel_size=2, stride=2,\n",
    "                               padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  #channel数变为n/2(非整数时向下取整)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])         #按照已有结论按二次方形式增长ResNet不同阶段的channel\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)#stride=2表示想把上一个layer传过来的size缩减为1/2\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((1, 1))  # output size = (1, 1) \n",
    "        self.lstm = nn.LSTM(256, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.to(device), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #预处理\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x) #残差网络\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x=torch.transpose(x,2,1)\n",
    "        #因为pytorch里lstm和conv1d的input sequence位置不一样，需要调整。\n",
    "        \n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "feature_channel=8\n",
    "hidden_size = 512  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_epochs = 600\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_lstm=ResNet1D_LSTM(BasicBlock, [3, 4, 3], num_classes=num_classes, feature_channel=feature_channel,\n",
    "                        hidden_size=hidden_size,num_layers=num_layers,device = torch.device(\"cpu\"))\n",
    "resnet_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#训练过程:相比前几个模型，resnet_lstm需要较多epoch才能训练成功, 所需训练的epoch数也与网络深度有关（梯度消失）\n",
    "#       可能需要对网络结构再进行改良\n",
    "total_step=0\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        #forward pass\n",
    "        outputs=resnet_lstm(images)\n",
    "        loss=loss_function(outputs,labels.long())\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step+=1\n",
    "        if (total_step)%10==0:#each 10 iterations is one epoch\n",
    "            print(\"Epoch [{}/{}],step[{}] Loss:{:.4f}\".format(epoch+1,num_epochs,total_step,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测过程, 和普通lstm效果差不多\n",
    "with torch.no_grad():\n",
    "    correct=0\n",
    "    total=0\n",
    "    for images, labels in test_loader:\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=resnet_lstm(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total=labels.size(0)\n",
    "        correct=(predicted==labels).sum().item()\n",
    "print('accuracy of the model on the test images: {}%'.format(100*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4层resnet（feature：64->128->256->512） + lstm 测试结果：\n",
    "#100epoch:0.9055\n",
    "#600epoch:0.9735\n",
    "#700epoch:0.9735\n",
    "#800epoch:0.9791\n",
    "#900epoch:0.9735\n",
    "#1000epoch:0.9596\n",
    "#3层resnet（feature：64->128->256） + lstm 测试结果：\n",
    "#100epoch:0.94\n",
    "#600epoch:0.9624\n",
    "#700epoch:0.9638\n",
    "#800epoch:0.9652\n",
    "#900epoch:0.9694\n",
    "#1000epoch:0.9666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
