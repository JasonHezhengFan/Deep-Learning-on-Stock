{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import classification_models\n",
    "from classification_models import GetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入训练数据\n",
    "a=np.load(\"new_train_data.npy\")\n",
    "b=a.reshape([a.shape[0],a.shape[1]*a.shape[2]])\n",
    "c=b[~np.isnan(b).any(axis=1),:]\n",
    "train_data=c.reshape([c.shape[0], a.shape[1],a.shape[2]])\n",
    "d=np.load(\"new_train_label.npy\").reshape(-1,1)\n",
    "train_label=d[~np.isnan(b).any(axis=1),:].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入valid数据\n",
    "a=np.load(\"new_valid_data.npy\")\n",
    "b=a.reshape([a.shape[0],a.shape[1]*a.shape[2]])\n",
    "c=b[~np.isnan(b).any(axis=1),:]\n",
    "valid_data=c.reshape([c.shape[0], a.shape[1],a.shape[2]])\n",
    "d=np.load(\"new_valid_label.npy\").reshape(-1,1)\n",
    "valid_label=d[~np.isnan(b).any(axis=1),:].reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_precision(model,images,labels,device,predict_type):\n",
    "    with torch.no_grad():\n",
    "        correct=0\n",
    "        total=0\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs=model(images)\n",
    "        _,predicted=torch.max(outputs.data,1)\n",
    "        total+=sum(predicted)\n",
    "        correct+=(sum(predicted*labels))\n",
    "        print('precision of the model on the'+predict_type+'data: {}%'.format(100*correct/total))\n",
    "    return predicted, 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIAL1\n",
    "#from classification_models import ResNet1D_LSTM\n",
    "#from classification_models import BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建ResNet-LSTM Model (Many-to-One) 相较于简单的cnn-lstm，resnet可以防止梯度消失\n",
    "#搭建第一类ResNet block\n",
    "class BasicBlock(nn.Module):#基本残差网络的一个模块类\n",
    "    expansion = 1#每一个residual block中不改变width,height,channel数，即增加的residual部分不需要做卷积处理\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)#stride=1,kernel_size=3,padding=1保证了data的\n",
    "                                                                                   #size不变 \n",
    "        self.bn1 = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:  #BasicBlock内不需要调整residual的height,width,channel\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#组建block成ResNet\n",
    "class ResNet1D_LSTM(nn.Module):\n",
    "    def __init__(self, block, blocks_num, num_classes, feature_channel,hidden_size, num_layers,device ):\n",
    "        super(ResNet1D_LSTM, self).__init__()\n",
    "        self.in_channel = 64\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_channel, out_channels=self.in_channel, kernel_size=2, stride=2,\n",
    "                               padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(self.in_channel)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  #channel数变为n/2(非整数时向下取整)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])         #按照已有结论按二次方形式增长ResNet不同阶段的channel\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)#stride=2表示想把上一个layer传过来的size缩减为1/2\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((1, 1))  # output size = (1, 1) \n",
    "        self.lstm = nn.LSTM(128, hidden_size, num_layers, batch_first=True)  # batch_first=True仅仅针对输入而言\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.to(device), mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel, channel))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) #预处理\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x) #残差网络\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        x=torch.transpose(x,2,1)\n",
    "        #因为pytorch里lstm和conv1d的input sequence位置不一样，需要调整。\n",
    "        \n",
    "        # 设置初始状态h_0与c_0的状态是初始的状态，一般设置为0，尺寸是,x.size(0)\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "\n",
    "        # Forward propagate RNN\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))  # 送入一个初始的x值，作为输入以及(h0, c0)\n",
    "\n",
    "        # Decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])  # output也是batch_first, 实际上h_n与c_n并不是batch_first\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "feature_channel=42\n",
    "hidden_size = 128  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "\n",
    "num_classes = 2\n",
    "batch_size = 512\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet1D_LSTM(\n",
       "  (conv1): Conv1d(42, 64, kernel_size=(2,), stride=(2,), bias=False)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=(1, 1))\n",
       "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_lstm=ResNet1D_LSTM(BasicBlock, [2,3], num_classes=num_classes, feature_channel=feature_channel,\n",
    "                        hidden_size=hidden_size,num_layers=num_layers,device = torch.device(\"cuda:1\"))\n",
    "resnet_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.transpose(train_data,(0,2,1))\n",
    "valid_data=np.transpose(valid_data,(0,2,1))\n",
    "train=GetLoader(train_data,train_label)\n",
    "valid=GetLoader(valid_data,valid_label)\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "valid_loader=torch.utils.data.DataLoader(dataset=valid,batch_size=valid_data.shape[0],shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397353, 42, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step=0\n",
    "train_precision=[]\n",
    "valid_precision=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet_lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/1000],step[1000] Loss:0.6850\n",
      "precision of the model on thetrainingdata: nan%\n",
      "precision of the model on thevalidationdata: nan%\n",
      "Epoch [3/1000],step[2000] Loss:0.6870\n",
      "precision of the model on thetrainingdata: nan%\n",
      "precision of the model on thevalidationdata: nan%\n",
      "Epoch [4/1000],step[3000] Loss:0.6930\n",
      "precision of the model on thetrainingdata: 54.54545593261719%\n",
      "precision of the model on thevalidationdata: 54.45134735107422%\n",
      "Epoch [6/1000],step[4000] Loss:0.6918\n",
      "precision of the model on thetrainingdata: 35.71428680419922%\n",
      "precision of the model on thevalidationdata: 54.21686935424805%\n",
      "Epoch [7/1000],step[5000] Loss:0.6940\n",
      "precision of the model on thetrainingdata: 52.94117736816406%\n",
      "precision of the model on thevalidationdata: 47.123661041259766%\n",
      "Epoch [8/1000],step[6000] Loss:0.6891\n",
      "precision of the model on thetrainingdata: 58.730159759521484%\n",
      "precision of the model on thevalidationdata: 52.16731262207031%\n",
      "Epoch [10/1000],step[7000] Loss:0.6894\n",
      "precision of the model on thetrainingdata: 48.83720779418945%\n",
      "precision of the model on thevalidationdata: 50.78086853027344%\n",
      "Epoch [11/1000],step[8000] Loss:0.6834\n",
      "precision of the model on thetrainingdata: 59.550559997558594%\n",
      "precision of the model on thevalidationdata: 48.85593795776367%\n",
      "Epoch [12/1000],step[9000] Loss:0.6881\n",
      "precision of the model on thetrainingdata: 54.09836196899414%\n",
      "precision of the model on thevalidationdata: 50.19633483886719%\n",
      "Epoch [13/1000],step[10000] Loss:0.6897\n",
      "precision of the model on thetrainingdata: 52.54237365722656%\n",
      "precision of the model on thevalidationdata: 48.52348327636719%\n",
      "Epoch [15/1000],step[11000] Loss:0.6895\n",
      "precision of the model on thetrainingdata: 63.75%\n",
      "precision of the model on thevalidationdata: 46.87654495239258%\n",
      "Epoch [16/1000],step[12000] Loss:0.6850\n",
      "precision of the model on thetrainingdata: 57.83132553100586%\n",
      "precision of the model on thevalidationdata: 48.27745056152344%\n",
      "Epoch [17/1000],step[13000] Loss:0.6900\n",
      "precision of the model on thetrainingdata: 50.72463607788086%\n",
      "precision of the model on thevalidationdata: 48.82379913330078%\n",
      "Epoch [19/1000],step[14000] Loss:0.6936\n",
      "precision of the model on thetrainingdata: 57.62711715698242%\n",
      "precision of the model on thevalidationdata: 48.68464279174805%\n",
      "Epoch [20/1000],step[15000] Loss:0.6829\n",
      "precision of the model on thetrainingdata: 53.93258285522461%\n",
      "precision of the model on thevalidationdata: 48.617034912109375%\n",
      "Epoch [21/1000],step[16000] Loss:0.6825\n",
      "precision of the model on thetrainingdata: 54.21686935424805%\n",
      "precision of the model on thevalidationdata: 48.535945892333984%\n",
      "Epoch [22/1000],step[17000] Loss:0.6858\n",
      "precision of the model on thetrainingdata: 59.61538314819336%\n",
      "precision of the model on thevalidationdata: 48.64562225341797%\n",
      "Epoch [24/1000],step[18000] Loss:0.6845\n",
      "precision of the model on thetrainingdata: 57.983192443847656%\n",
      "precision of the model on thevalidationdata: 48.469207763671875%\n",
      "Epoch [25/1000],step[19000] Loss:0.6851\n",
      "precision of the model on thetrainingdata: 61.53845977783203%\n",
      "precision of the model on thevalidationdata: 48.52428436279297%\n",
      "Epoch [26/1000],step[20000] Loss:0.6854\n",
      "precision of the model on thetrainingdata: 49.03845977783203%\n",
      "precision of the model on thevalidationdata: 48.9515266418457%\n",
      "Epoch [28/1000],step[21000] Loss:0.6795\n",
      "precision of the model on thetrainingdata: 58.441558837890625%\n",
      "precision of the model on thevalidationdata: 48.72724914550781%\n",
      "Epoch [29/1000],step[22000] Loss:0.6818\n",
      "precision of the model on thetrainingdata: 46.47887420654297%\n",
      "precision of the model on thevalidationdata: 48.108421325683594%\n",
      "Epoch [30/1000],step[23000] Loss:0.6787\n",
      "precision of the model on thetrainingdata: 57.657657623291016%\n",
      "precision of the model on thevalidationdata: 48.90436553955078%\n",
      "Epoch [31/1000],step[24000] Loss:0.6770\n",
      "precision of the model on thetrainingdata: 54.1984748840332%\n",
      "precision of the model on thevalidationdata: 49.116912841796875%\n",
      "Epoch [33/1000],step[25000] Loss:0.6817\n",
      "precision of the model on thetrainingdata: 61.26760482788086%\n",
      "precision of the model on thevalidationdata: 48.11363983154297%\n",
      "Epoch [34/1000],step[26000] Loss:0.6874\n",
      "precision of the model on thetrainingdata: 51.49253845214844%\n",
      "precision of the model on thevalidationdata: 47.19456100463867%\n",
      "Epoch [35/1000],step[27000] Loss:0.6781\n",
      "precision of the model on thetrainingdata: 54.9549560546875%\n",
      "precision of the model on thevalidationdata: 48.19392395019531%\n",
      "Epoch [37/1000],step[28000] Loss:0.6892\n",
      "precision of the model on thetrainingdata: 54.032257080078125%\n",
      "precision of the model on thevalidationdata: 48.68683624267578%\n",
      "Epoch [38/1000],step[29000] Loss:0.6707\n",
      "precision of the model on thetrainingdata: 56.66666793823242%\n",
      "precision of the model on thevalidationdata: 49.10496139526367%\n",
      "Epoch [39/1000],step[30000] Loss:0.6837\n",
      "precision of the model on thetrainingdata: 54.30107498168945%\n",
      "precision of the model on thevalidationdata: 48.13013458251953%\n",
      "Epoch [40/1000],step[31000] Loss:0.6680\n",
      "precision of the model on thetrainingdata: 62.33766174316406%\n",
      "precision of the model on thevalidationdata: 48.39217758178711%\n",
      "Epoch [42/1000],step[32000] Loss:0.6755\n",
      "precision of the model on thetrainingdata: 54.34782791137695%\n",
      "precision of the model on thevalidationdata: 48.79814529418945%\n",
      "Epoch [43/1000],step[33000] Loss:0.6628\n",
      "precision of the model on thetrainingdata: 58.5%\n",
      "precision of the model on thevalidationdata: 48.62638854980469%\n",
      "Epoch [44/1000],step[34000] Loss:0.6553\n",
      "precision of the model on thetrainingdata: 66.4383544921875%\n",
      "precision of the model on thevalidationdata: 48.10316848754883%\n",
      "Epoch [46/1000],step[35000] Loss:0.6792\n",
      "precision of the model on thetrainingdata: 51.1363639831543%\n",
      "precision of the model on thevalidationdata: 48.133426666259766%\n",
      "Epoch [47/1000],step[36000] Loss:0.6575\n",
      "precision of the model on thetrainingdata: 61.212120056152344%\n",
      "precision of the model on thevalidationdata: 48.542110443115234%\n",
      "Epoch [48/1000],step[37000] Loss:0.6984\n",
      "precision of the model on thetrainingdata: 54.72636795043945%\n",
      "precision of the model on thevalidationdata: 48.72581481933594%\n",
      "Epoch [49/1000],step[38000] Loss:0.6691\n",
      "precision of the model on thetrainingdata: 62.721893310546875%\n",
      "precision of the model on thevalidationdata: 47.59876251220703%\n",
      "Epoch [51/1000],step[39000] Loss:0.6489\n",
      "precision of the model on thetrainingdata: 63.21839141845703%\n",
      "precision of the model on thevalidationdata: 48.01397705078125%\n",
      "Epoch [52/1000],step[40000] Loss:0.6596\n",
      "precision of the model on thetrainingdata: 65.51724243164062%\n",
      "precision of the model on thevalidationdata: 48.30046463012695%\n",
      "Epoch [53/1000],step[41000] Loss:0.6485\n",
      "precision of the model on thetrainingdata: 57.92349624633789%\n",
      "precision of the model on thevalidationdata: 48.047889709472656%\n",
      "Epoch [55/1000],step[42000] Loss:0.6242\n",
      "precision of the model on thetrainingdata: 62.30366516113281%\n",
      "precision of the model on thevalidationdata: 48.14927291870117%\n",
      "Epoch [56/1000],step[43000] Loss:0.6296\n",
      "precision of the model on thetrainingdata: 60.227272033691406%\n",
      "precision of the model on thevalidationdata: 48.183692932128906%\n",
      "Epoch [57/1000],step[44000] Loss:0.6224\n",
      "precision of the model on thetrainingdata: 61.42856979370117%\n",
      "precision of the model on thevalidationdata: 48.13982391357422%\n",
      "Epoch [58/1000],step[45000] Loss:0.6218\n",
      "precision of the model on thetrainingdata: 66.01941680908203%\n",
      "precision of the model on thevalidationdata: 48.06944274902344%\n",
      "Epoch [60/1000],step[46000] Loss:0.5948\n",
      "precision of the model on thetrainingdata: 72.7272720336914%\n",
      "precision of the model on thevalidationdata: 48.15073013305664%\n",
      "Epoch [61/1000],step[47000] Loss:0.5829\n",
      "precision of the model on thetrainingdata: 73.16017150878906%\n",
      "precision of the model on thevalidationdata: 47.804786682128906%\n",
      "Epoch [62/1000],step[48000] Loss:0.6148\n",
      "precision of the model on thetrainingdata: 64.53202056884766%\n",
      "precision of the model on thevalidationdata: 48.349308013916016%\n",
      "Epoch [64/1000],step[49000] Loss:0.5864\n",
      "precision of the model on thetrainingdata: 69.71428680419922%\n",
      "precision of the model on thevalidationdata: 48.19978713989258%\n",
      "Epoch [65/1000],step[50000] Loss:0.5976\n",
      "precision of the model on thetrainingdata: 66.99507141113281%\n",
      "precision of the model on thevalidationdata: 48.12632751464844%\n",
      "Epoch [66/1000],step[51000] Loss:0.5551\n",
      "precision of the model on thetrainingdata: 71.07843017578125%\n",
      "precision of the model on thevalidationdata: 47.848472595214844%\n",
      "Epoch [67/1000],step[52000] Loss:0.5843\n",
      "precision of the model on thetrainingdata: 77.07317352294922%\n",
      "precision of the model on thevalidationdata: 47.95893478393555%\n",
      "Epoch [69/1000],step[53000] Loss:0.6175\n",
      "precision of the model on thetrainingdata: 73.5426025390625%\n",
      "precision of the model on thevalidationdata: 47.741695404052734%\n",
      "Epoch [70/1000],step[54000] Loss:0.5726\n",
      "precision of the model on thetrainingdata: 67.5%\n",
      "precision of the model on thevalidationdata: 47.653236389160156%\n",
      "Epoch [71/1000],step[55000] Loss:0.5766\n",
      "precision of the model on thetrainingdata: 68.62744903564453%\n",
      "precision of the model on thevalidationdata: 48.210933685302734%\n",
      "Epoch [73/1000],step[56000] Loss:0.5565\n",
      "precision of the model on thetrainingdata: 67.00507354736328%\n",
      "precision of the model on thevalidationdata: 48.24742126464844%\n",
      "Epoch [74/1000],step[57000] Loss:0.5302\n",
      "precision of the model on thetrainingdata: 71.69811248779297%\n",
      "precision of the model on thevalidationdata: 48.47603225708008%\n",
      "Epoch [75/1000],step[58000] Loss:0.5400\n",
      "precision of the model on thetrainingdata: 66.98113250732422%\n",
      "precision of the model on thevalidationdata: 48.035797119140625%\n",
      "Epoch [76/1000],step[59000] Loss:0.5488\n",
      "precision of the model on thetrainingdata: 70.19230651855469%\n",
      "precision of the model on thevalidationdata: 47.84764099121094%\n",
      "Epoch [78/1000],step[60000] Loss:0.5127\n",
      "precision of the model on thetrainingdata: 69.29824829101562%\n",
      "precision of the model on thevalidationdata: 48.301597595214844%\n",
      "Epoch [79/1000],step[61000] Loss:0.5191\n",
      "precision of the model on thetrainingdata: 72.56636810302734%\n",
      "precision of the model on thevalidationdata: 48.112159729003906%\n",
      "Epoch [80/1000],step[62000] Loss:0.5029\n",
      "precision of the model on thetrainingdata: 80.09259033203125%\n",
      "precision of the model on thevalidationdata: 48.25327682495117%\n",
      "Epoch [82/1000],step[63000] Loss:0.4926\n",
      "precision of the model on thetrainingdata: 73.66071319580078%\n",
      "precision of the model on thevalidationdata: 47.86691665649414%\n",
      "Epoch [83/1000],step[64000] Loss:0.4824\n",
      "precision of the model on thetrainingdata: 81.69013977050781%\n",
      "precision of the model on thevalidationdata: 48.63066101074219%\n",
      "Epoch [84/1000],step[65000] Loss:0.5175\n",
      "precision of the model on thetrainingdata: 70.1680679321289%\n",
      "precision of the model on thevalidationdata: 47.885650634765625%\n",
      "Epoch [85/1000],step[66000] Loss:0.5004\n",
      "precision of the model on thetrainingdata: 77.19298553466797%\n",
      "precision of the model on thevalidationdata: 48.48414993286133%\n",
      "Epoch [87/1000],step[67000] Loss:0.4713\n",
      "precision of the model on thetrainingdata: 79.72350311279297%\n",
      "precision of the model on thevalidationdata: 48.22966003417969%\n",
      "Epoch [88/1000],step[68000] Loss:0.4942\n",
      "precision of the model on thetrainingdata: 74.38016510009766%\n",
      "precision of the model on thevalidationdata: 48.23965835571289%\n",
      "Epoch [89/1000],step[69000] Loss:0.4922\n",
      "precision of the model on thetrainingdata: 74.38016510009766%\n",
      "precision of the model on thevalidationdata: 48.2132568359375%\n",
      "Epoch [91/1000],step[70000] Loss:0.4599\n",
      "precision of the model on thetrainingdata: 80.45454406738281%\n",
      "precision of the model on thevalidationdata: 48.4084358215332%\n",
      "Epoch [92/1000],step[71000] Loss:0.4305\n",
      "precision of the model on thetrainingdata: 78.50877380371094%\n",
      "precision of the model on thevalidationdata: 48.157203674316406%\n",
      "Epoch [93/1000],step[72000] Loss:0.4541\n",
      "precision of the model on thetrainingdata: 81.01265716552734%\n",
      "precision of the model on thevalidationdata: 48.304866790771484%\n",
      "Epoch [94/1000],step[73000] Loss:0.4537\n",
      "precision of the model on thetrainingdata: 76.47058868408203%\n",
      "precision of the model on thevalidationdata: 47.66487503051758%\n",
      "Epoch [96/1000],step[74000] Loss:0.4209\n",
      "precision of the model on thetrainingdata: 81.85653686523438%\n",
      "precision of the model on thevalidationdata: 48.224517822265625%\n",
      "Epoch [97/1000],step[75000] Loss:0.4122\n",
      "precision of the model on thetrainingdata: 80.25750732421875%\n",
      "precision of the model on thevalidationdata: 47.84334182739258%\n",
      "Epoch [98/1000],step[76000] Loss:0.4046\n",
      "precision of the model on thetrainingdata: 81.98197937011719%\n",
      "precision of the model on thevalidationdata: 48.27360534667969%\n",
      "Epoch [100/1000],step[77000] Loss:0.4135\n",
      "precision of the model on thetrainingdata: 83.47457885742188%\n",
      "precision of the model on thevalidationdata: 47.860870361328125%\n",
      "Epoch [101/1000],step[78000] Loss:0.3912\n",
      "precision of the model on thetrainingdata: 83.11111450195312%\n",
      "precision of the model on thevalidationdata: 47.958675384521484%\n",
      "Epoch [102/1000],step[79000] Loss:0.4237\n",
      "precision of the model on thetrainingdata: 79.73567962646484%\n",
      "precision of the model on thevalidationdata: 47.77821350097656%\n",
      "Epoch [103/1000],step[80000] Loss:0.4074\n",
      "precision of the model on thetrainingdata: 87.12446594238281%\n",
      "precision of the model on thevalidationdata: 48.167301177978516%\n",
      "Epoch [105/1000],step[81000] Loss:0.3890\n",
      "precision of the model on thetrainingdata: 80.76923370361328%\n",
      "precision of the model on thevalidationdata: 47.95936584472656%\n",
      "Epoch [106/1000],step[82000] Loss:0.3773\n",
      "precision of the model on thetrainingdata: 86.32075500488281%\n",
      "precision of the model on thevalidationdata: 48.170536041259766%\n",
      "Epoch [107/1000],step[83000] Loss:0.3989\n",
      "precision of the model on thetrainingdata: 84.05796813964844%\n",
      "precision of the model on thevalidationdata: 47.92236328125%\n",
      "Epoch [109/1000],step[84000] Loss:0.4002\n",
      "precision of the model on thetrainingdata: 84.54545593261719%\n",
      "precision of the model on thevalidationdata: 47.780906677246094%\n",
      "Epoch [110/1000],step[85000] Loss:0.3941\n",
      "precision of the model on thetrainingdata: 85.02202606201172%\n",
      "precision of the model on thevalidationdata: 48.02097702026367%\n",
      "Epoch [111/1000],step[86000] Loss:0.3895\n",
      "precision of the model on thetrainingdata: 81.10598754882812%\n",
      "precision of the model on thevalidationdata: 48.54932403564453%\n",
      "Epoch [112/1000],step[87000] Loss:0.3759\n",
      "precision of the model on thetrainingdata: 89.5734634399414%\n",
      "precision of the model on thevalidationdata: 48.33683776855469%\n",
      "Epoch [114/1000],step[88000] Loss:0.3679\n",
      "precision of the model on thetrainingdata: 82.9959487915039%\n",
      "precision of the model on thevalidationdata: 48.36115646362305%\n",
      "Epoch [115/1000],step[89000] Loss:0.3611\n",
      "precision of the model on thetrainingdata: 83.56807708740234%\n",
      "precision of the model on thevalidationdata: 48.12128829956055%\n",
      "Epoch [116/1000],step[90000] Loss:0.3721\n",
      "precision of the model on thetrainingdata: 81.04265594482422%\n",
      "precision of the model on thevalidationdata: 48.148990631103516%\n",
      "Epoch [118/1000],step[91000] Loss:0.3805\n",
      "precision of the model on thetrainingdata: 81.22065734863281%\n",
      "precision of the model on thevalidationdata: 48.163421630859375%\n",
      "Epoch [119/1000],step[92000] Loss:0.3539\n",
      "precision of the model on thetrainingdata: 83.02751922607422%\n",
      "precision of the model on thevalidationdata: 48.19845199584961%\n",
      "Epoch [120/1000],step[93000] Loss:0.3325\n",
      "precision of the model on thetrainingdata: 86.36363983154297%\n",
      "precision of the model on thevalidationdata: 47.99697494506836%\n",
      "Epoch [121/1000],step[94000] Loss:0.3277\n",
      "precision of the model on thetrainingdata: 85.06787109375%\n",
      "precision of the model on thevalidationdata: 48.42352294921875%\n",
      "Epoch [123/1000],step[95000] Loss:0.3108\n",
      "precision of the model on thetrainingdata: 88.70292663574219%\n",
      "precision of the model on thevalidationdata: 48.46369171142578%\n",
      "Epoch [124/1000],step[96000] Loss:0.3954\n",
      "precision of the model on thetrainingdata: 82.47012329101562%\n",
      "precision of the model on thevalidationdata: 48.340248107910156%\n",
      "Epoch [125/1000],step[97000] Loss:0.3335\n",
      "precision of the model on thetrainingdata: 90.17857360839844%\n",
      "precision of the model on thevalidationdata: 48.5443229675293%\n",
      "Epoch [127/1000],step[98000] Loss:0.2906\n",
      "precision of the model on thetrainingdata: 88.0%\n",
      "precision of the model on thevalidationdata: 48.44403076171875%\n",
      "Epoch [128/1000],step[99000] Loss:0.3384\n",
      "precision of the model on thetrainingdata: 87.8787841796875%\n",
      "precision of the model on thevalidationdata: 48.22294998168945%\n",
      "Epoch [129/1000],step[100000] Loss:0.3045\n",
      "precision of the model on thetrainingdata: 84.64730072021484%\n",
      "precision of the model on thevalidationdata: 48.260372161865234%\n",
      "Epoch [130/1000],step[101000] Loss:0.2925\n",
      "precision of the model on thetrainingdata: 92.035400390625%\n",
      "precision of the model on thevalidationdata: 48.426326751708984%\n",
      "Epoch [132/1000],step[102000] Loss:0.3608\n",
      "precision of the model on thetrainingdata: 83.60655975341797%\n",
      "precision of the model on thevalidationdata: 48.53684997558594%\n",
      "Epoch [133/1000],step[103000] Loss:0.3064\n",
      "precision of the model on thetrainingdata: 89.6551742553711%\n",
      "precision of the model on thevalidationdata: 48.12925338745117%\n",
      "Epoch [134/1000],step[104000] Loss:0.3079\n",
      "precision of the model on thetrainingdata: 85.8974380493164%\n",
      "precision of the model on thevalidationdata: 48.39112854003906%\n",
      "Epoch [136/1000],step[105000] Loss:0.2820\n",
      "precision of the model on thetrainingdata: 88.15789794921875%\n",
      "precision of the model on thevalidationdata: 48.07051467895508%\n",
      "Epoch [137/1000],step[106000] Loss:0.3035\n",
      "precision of the model on thetrainingdata: 84.73895263671875%\n",
      "precision of the model on thevalidationdata: 48.318016052246094%\n",
      "Epoch [138/1000],step[107000] Loss:0.3611\n",
      "precision of the model on thetrainingdata: 86.55461883544922%\n",
      "precision of the model on thevalidationdata: 47.814170837402344%\n",
      "Epoch [139/1000],step[108000] Loss:0.2900\n",
      "precision of the model on thetrainingdata: 92.34449768066406%\n",
      "precision of the model on thevalidationdata: 48.08815383911133%\n",
      "Epoch [141/1000],step[109000] Loss:0.2558\n",
      "precision of the model on thetrainingdata: 94.17040252685547%\n",
      "precision of the model on thevalidationdata: 47.96868896484375%\n",
      "Epoch [142/1000],step[110000] Loss:0.2739\n",
      "precision of the model on thetrainingdata: 92.82511138916016%\n",
      "precision of the model on thevalidationdata: 48.29378128051758%\n",
      "Epoch [143/1000],step[111000] Loss:0.2967\n",
      "precision of the model on thetrainingdata: 86.12245178222656%\n",
      "precision of the model on thevalidationdata: 48.20392608642578%\n",
      "Epoch [145/1000],step[112000] Loss:0.2807\n",
      "precision of the model on thetrainingdata: 90.95022583007812%\n",
      "precision of the model on thevalidationdata: 48.39246368408203%\n",
      "Epoch [146/1000],step[113000] Loss:0.3041\n",
      "precision of the model on thetrainingdata: 89.40677642822266%\n",
      "precision of the model on thevalidationdata: 48.65224838256836%\n",
      "Epoch [147/1000],step[114000] Loss:0.2822\n",
      "precision of the model on thetrainingdata: 91.089111328125%\n",
      "precision of the model on thevalidationdata: 48.239891052246094%\n",
      "Epoch [149/1000],step[115000] Loss:0.5346\n",
      "precision of the model on thetrainingdata: 78.13953399658203%\n",
      "precision of the model on thevalidationdata: 48.36128616333008%\n",
      "Epoch [150/1000],step[116000] Loss:0.2417\n",
      "precision of the model on thetrainingdata: 91.2844009399414%\n",
      "precision of the model on thevalidationdata: 48.12356948852539%\n",
      "Epoch [151/1000],step[117000] Loss:0.3009\n",
      "precision of the model on thetrainingdata: 92.59259033203125%\n",
      "precision of the model on thevalidationdata: 48.09377670288086%\n",
      "Epoch [152/1000],step[118000] Loss:0.3256\n",
      "precision of the model on thetrainingdata: 90.37657165527344%\n",
      "precision of the model on thevalidationdata: 48.080196380615234%\n",
      "Epoch [154/1000],step[119000] Loss:0.2424\n",
      "precision of the model on thetrainingdata: 90.63829803466797%\n",
      "precision of the model on thevalidationdata: 48.028404235839844%\n",
      "Epoch [155/1000],step[120000] Loss:0.2880\n",
      "precision of the model on thetrainingdata: 90.98712158203125%\n",
      "precision of the model on thevalidationdata: 48.028343200683594%\n",
      "Epoch [156/1000],step[121000] Loss:0.2659\n",
      "precision of the model on thetrainingdata: 91.5929183959961%\n",
      "precision of the model on thevalidationdata: 48.08965301513672%\n",
      "Epoch [158/1000],step[122000] Loss:0.4092\n",
      "precision of the model on thetrainingdata: 83.92857360839844%\n",
      "precision of the model on thevalidationdata: 48.20404052734375%\n",
      "Epoch [159/1000],step[123000] Loss:0.2696\n",
      "precision of the model on thetrainingdata: 89.83739471435547%\n",
      "precision of the model on thevalidationdata: 48.567752838134766%\n",
      "Epoch [160/1000],step[124000] Loss:0.2899\n",
      "precision of the model on thetrainingdata: 90.90908813476562%\n",
      "precision of the model on thevalidationdata: 48.15011215209961%\n",
      "Epoch [161/1000],step[125000] Loss:0.2707\n",
      "precision of the model on thetrainingdata: 89.02953338623047%\n",
      "precision of the model on thevalidationdata: 48.12678909301758%\n",
      "Epoch [163/1000],step[126000] Loss:0.2090\n",
      "precision of the model on thetrainingdata: 91.77489471435547%\n",
      "precision of the model on thevalidationdata: 48.39586639404297%\n",
      "Epoch [164/1000],step[127000] Loss:0.2570\n",
      "precision of the model on thetrainingdata: 90.16393280029297%\n",
      "precision of the model on thevalidationdata: 48.155921936035156%\n",
      "Epoch [165/1000],step[128000] Loss:0.2331\n",
      "precision of the model on thetrainingdata: 93.25843048095703%\n",
      "precision of the model on thevalidationdata: 48.06400680541992%\n",
      "Epoch [167/1000],step[129000] Loss:0.2865\n",
      "precision of the model on thetrainingdata: 87.67772674560547%\n",
      "precision of the model on thevalidationdata: 48.193355560302734%\n",
      "Epoch [168/1000],step[130000] Loss:0.2272\n",
      "precision of the model on thetrainingdata: 91.9148941040039%\n",
      "precision of the model on thevalidationdata: 48.059207916259766%\n",
      "Epoch [169/1000],step[131000] Loss:0.2227\n",
      "precision of the model on thetrainingdata: 90.12345886230469%\n",
      "precision of the model on thevalidationdata: 48.22078323364258%\n",
      "Epoch [170/1000],step[132000] Loss:0.2678\n",
      "precision of the model on thetrainingdata: 88.0%\n",
      "precision of the model on thevalidationdata: 48.043128967285156%\n",
      "Epoch [172/1000],step[133000] Loss:0.2506\n",
      "precision of the model on thetrainingdata: 92.36947631835938%\n",
      "precision of the model on thevalidationdata: 48.002933502197266%\n",
      "Epoch [173/1000],step[134000] Loss:0.2072\n",
      "precision of the model on thetrainingdata: 93.1192626953125%\n",
      "precision of the model on thevalidationdata: 48.31010437011719%\n",
      "Epoch [174/1000],step[135000] Loss:0.1900\n",
      "precision of the model on thetrainingdata: 96.0%\n",
      "precision of the model on thevalidationdata: 48.53957748413086%\n",
      "Epoch [176/1000],step[136000] Loss:0.3305\n",
      "precision of the model on thetrainingdata: 83.26692962646484%\n",
      "precision of the model on thevalidationdata: 48.072391510009766%\n",
      "Epoch [177/1000],step[137000] Loss:0.2362\n",
      "precision of the model on thetrainingdata: 90.51724243164062%\n",
      "precision of the model on thevalidationdata: 48.024208068847656%\n",
      "Epoch [178/1000],step[138000] Loss:0.2567\n",
      "precision of the model on thetrainingdata: 91.80327606201172%\n",
      "precision of the model on thevalidationdata: 48.249061584472656%\n",
      "Epoch [179/1000],step[139000] Loss:0.2514\n",
      "precision of the model on thetrainingdata: 90.47618865966797%\n",
      "precision of the model on thevalidationdata: 47.814842224121094%\n",
      "Epoch [181/1000],step[140000] Loss:0.1951\n",
      "precision of the model on thetrainingdata: 93.52227020263672%\n",
      "precision of the model on thevalidationdata: 48.13725662231445%\n",
      "Epoch [182/1000],step[141000] Loss:0.1925\n",
      "precision of the model on thetrainingdata: 89.40092468261719%\n",
      "precision of the model on thevalidationdata: 48.267024993896484%\n",
      "Epoch [183/1000],step[142000] Loss:0.2523\n",
      "precision of the model on thetrainingdata: 92.76596069335938%\n",
      "precision of the model on thevalidationdata: 47.743839263916016%\n",
      "Epoch [185/1000],step[143000] Loss:0.2400\n",
      "precision of the model on thetrainingdata: 90.0862045288086%\n",
      "precision of the model on thevalidationdata: 48.37627029418945%\n",
      "Epoch [186/1000],step[144000] Loss:0.2154\n",
      "precision of the model on thetrainingdata: 94.54545593261719%\n",
      "precision of the model on thevalidationdata: 48.2924690246582%\n",
      "Epoch [187/1000],step[145000] Loss:0.2209\n",
      "precision of the model on thetrainingdata: 94.06392669677734%\n",
      "precision of the model on thevalidationdata: 48.12390899658203%\n",
      "Epoch [188/1000],step[146000] Loss:0.1875\n",
      "precision of the model on thetrainingdata: 92.7966079711914%\n",
      "precision of the model on thevalidationdata: 48.18942642211914%\n",
      "Epoch [190/1000],step[147000] Loss:0.2389\n",
      "precision of the model on thetrainingdata: 91.59664154052734%\n",
      "precision of the model on thevalidationdata: 47.9574089050293%\n",
      "Epoch [191/1000],step[148000] Loss:0.2271\n",
      "precision of the model on thetrainingdata: 94.67213439941406%\n",
      "precision of the model on thevalidationdata: 48.467350006103516%\n",
      "Epoch [192/1000],step[149000] Loss:0.1985\n",
      "precision of the model on thetrainingdata: 92.4000015258789%\n",
      "precision of the model on thevalidationdata: 48.27616882324219%\n",
      "Epoch [194/1000],step[150000] Loss:0.1975\n",
      "precision of the model on thetrainingdata: 94.66666412353516%\n",
      "precision of the model on thevalidationdata: 48.10408401489258%\n",
      "Epoch [195/1000],step[151000] Loss:0.2234\n",
      "precision of the model on thetrainingdata: 92.47787475585938%\n",
      "precision of the model on thevalidationdata: 48.28579330444336%\n",
      "Epoch [196/1000],step[152000] Loss:0.2144\n",
      "precision of the model on thetrainingdata: 93.06122589111328%\n",
      "precision of the model on thevalidationdata: 48.377201080322266%\n",
      "Epoch [197/1000],step[153000] Loss:0.2183\n",
      "precision of the model on thetrainingdata: 91.90283203125%\n",
      "precision of the model on thevalidationdata: 48.325565338134766%\n",
      "Epoch [199/1000],step[154000] Loss:0.2440\n",
      "precision of the model on thetrainingdata: 93.65079498291016%\n",
      "precision of the model on thevalidationdata: 48.59035110473633%\n",
      "Epoch [200/1000],step[155000] Loss:0.1828\n",
      "precision of the model on thetrainingdata: 95.25862121582031%\n",
      "precision of the model on thevalidationdata: 48.39067077636719%\n",
      "Epoch [201/1000],step[156000] Loss:0.2078\n",
      "precision of the model on thetrainingdata: 94.59459686279297%\n",
      "precision of the model on thevalidationdata: 48.13667297363281%\n",
      "Epoch [203/1000],step[157000] Loss:0.2055\n",
      "precision of the model on thetrainingdata: 92.85713958740234%\n",
      "precision of the model on thevalidationdata: 47.9183349609375%\n",
      "Epoch [204/1000],step[158000] Loss:0.1898\n",
      "precision of the model on thetrainingdata: 94.23868560791016%\n",
      "precision of the model on thevalidationdata: 48.31593704223633%\n",
      "Epoch [205/1000],step[159000] Loss:0.1756\n",
      "precision of the model on thetrainingdata: 95.76271057128906%\n",
      "precision of the model on thevalidationdata: 48.29708480834961%\n",
      "Epoch [206/1000],step[160000] Loss:0.1928\n",
      "precision of the model on thetrainingdata: 91.015625%\n",
      "precision of the model on thevalidationdata: 48.085750579833984%\n",
      "Epoch [208/1000],step[161000] Loss:0.1809\n",
      "precision of the model on thetrainingdata: 94.09282684326172%\n",
      "precision of the model on thevalidationdata: 48.40459442138672%\n",
      "Epoch [209/1000],step[162000] Loss:0.2169\n",
      "precision of the model on thetrainingdata: 93.10344696044922%\n",
      "precision of the model on thevalidationdata: 48.52000427246094%\n",
      "Epoch [210/1000],step[163000] Loss:0.2099\n",
      "precision of the model on thetrainingdata: 93.36099243164062%\n",
      "precision of the model on thevalidationdata: 48.37305450439453%\n",
      "Epoch [212/1000],step[164000] Loss:0.2379\n",
      "precision of the model on thetrainingdata: 88.30188751220703%\n",
      "precision of the model on thevalidationdata: 48.24458312988281%\n",
      "Epoch [213/1000],step[165000] Loss:0.1365\n",
      "precision of the model on thetrainingdata: 96.20252990722656%\n",
      "precision of the model on thevalidationdata: 48.400489807128906%\n",
      "Epoch [214/1000],step[166000] Loss:0.2496\n",
      "precision of the model on thetrainingdata: 90.66666412353516%\n",
      "precision of the model on thevalidationdata: 48.172306060791016%\n",
      "Epoch [215/1000],step[167000] Loss:0.1787\n",
      "precision of the model on thetrainingdata: 93.72384643554688%\n",
      "precision of the model on thevalidationdata: 48.06407928466797%\n",
      "Epoch [217/1000],step[168000] Loss:0.1412\n",
      "precision of the model on thetrainingdata: 94.84978485107422%\n",
      "precision of the model on thevalidationdata: 48.32460021972656%\n",
      "Epoch [218/1000],step[169000] Loss:0.2067\n",
      "precision of the model on thetrainingdata: 95.25862121582031%\n",
      "precision of the model on thevalidationdata: 48.449317932128906%\n",
      "Epoch [219/1000],step[170000] Loss:0.1545\n",
      "precision of the model on thetrainingdata: 97.36842346191406%\n",
      "precision of the model on thevalidationdata: 48.525054931640625%\n",
      "Epoch [221/1000],step[171000] Loss:0.1840\n",
      "precision of the model on thetrainingdata: 93.08943176269531%\n",
      "precision of the model on thevalidationdata: 48.161861419677734%\n",
      "Epoch [222/1000],step[172000] Loss:0.1828\n",
      "precision of the model on thetrainingdata: 95.06172943115234%\n",
      "precision of the model on thevalidationdata: 48.41545486450195%\n",
      "Epoch [223/1000],step[173000] Loss:0.3009\n",
      "precision of the model on thetrainingdata: 92.85713958740234%\n",
      "precision of the model on thevalidationdata: 48.36827850341797%\n",
      "Epoch [224/1000],step[174000] Loss:0.2044\n",
      "precision of the model on thetrainingdata: 92.36947631835938%\n",
      "precision of the model on thevalidationdata: 48.106266021728516%\n",
      "Epoch [226/1000],step[175000] Loss:0.1727\n",
      "precision of the model on thetrainingdata: 94.16666412353516%\n",
      "precision of the model on thevalidationdata: 48.61304473876953%\n",
      "Epoch [227/1000],step[176000] Loss:0.1607\n",
      "precision of the model on thetrainingdata: 93.85246276855469%\n",
      "precision of the model on thevalidationdata: 48.1912841796875%\n",
      "Epoch [228/1000],step[177000] Loss:0.2064\n",
      "precision of the model on thetrainingdata: 96.32653045654297%\n",
      "precision of the model on thevalidationdata: 48.56358337402344%\n",
      "Epoch [230/1000],step[178000] Loss:0.1735\n",
      "precision of the model on thetrainingdata: 93.80165100097656%\n",
      "precision of the model on thevalidationdata: 48.386817932128906%\n",
      "Epoch [231/1000],step[179000] Loss:0.1542\n",
      "precision of the model on thetrainingdata: 95.56451416015625%\n",
      "precision of the model on thevalidationdata: 48.491512298583984%\n",
      "Epoch [232/1000],step[180000] Loss:0.1389\n",
      "precision of the model on thetrainingdata: 96.73469543457031%\n",
      "precision of the model on thevalidationdata: 48.22755813598633%\n",
      "Epoch [233/1000],step[181000] Loss:0.1587\n",
      "precision of the model on thetrainingdata: 96.07843017578125%\n",
      "precision of the model on thevalidationdata: 48.21451950073242%\n",
      "Epoch [235/1000],step[182000] Loss:0.1319\n",
      "precision of the model on thetrainingdata: 95.29412078857422%\n",
      "precision of the model on thevalidationdata: 48.13695526123047%\n",
      "Epoch [236/1000],step[183000] Loss:0.1762\n",
      "precision of the model on thetrainingdata: 91.89189147949219%\n",
      "precision of the model on thevalidationdata: 48.13492965698242%\n",
      "Epoch [237/1000],step[184000] Loss:0.1996\n",
      "precision of the model on thetrainingdata: 92.18106842041016%\n",
      "precision of the model on thevalidationdata: 48.33559799194336%\n",
      "Epoch [239/1000],step[185000] Loss:0.1537\n",
      "precision of the model on thetrainingdata: 94.62809753417969%\n",
      "precision of the model on thevalidationdata: 48.098941802978516%\n",
      "Epoch [240/1000],step[186000] Loss:0.1717\n",
      "precision of the model on thetrainingdata: 92.80000305175781%\n",
      "precision of the model on thevalidationdata: 48.25386047363281%\n",
      "Epoch [241/1000],step[187000] Loss:0.1400\n",
      "precision of the model on thetrainingdata: 97.96747589111328%\n",
      "precision of the model on thevalidationdata: 47.95421600341797%\n",
      "Epoch [242/1000],step[188000] Loss:0.2121\n",
      "precision of the model on thetrainingdata: 94.11764526367188%\n",
      "precision of the model on thevalidationdata: 48.43563461303711%\n",
      "Epoch [244/1000],step[189000] Loss:0.1483\n",
      "precision of the model on thetrainingdata: 95.92760467529297%\n",
      "precision of the model on thevalidationdata: 48.27084732055664%\n",
      "Epoch [245/1000],step[190000] Loss:0.2688\n",
      "precision of the model on thetrainingdata: 94.5147705078125%\n",
      "precision of the model on thevalidationdata: 48.410579681396484%\n",
      "Epoch [246/1000],step[191000] Loss:0.1268\n",
      "precision of the model on thetrainingdata: 96.53679656982422%\n",
      "precision of the model on thevalidationdata: 48.45233917236328%\n",
      "Epoch [248/1000],step[192000] Loss:0.1563\n",
      "precision of the model on thetrainingdata: 96.80000305175781%\n",
      "precision of the model on thevalidationdata: 48.38809585571289%\n",
      "Epoch [249/1000],step[193000] Loss:0.1395\n",
      "precision of the model on thetrainingdata: 97.17742156982422%\n",
      "precision of the model on thevalidationdata: 48.34955596923828%\n",
      "Epoch [250/1000],step[194000] Loss:0.1471\n",
      "precision of the model on thetrainingdata: 95.91836547851562%\n",
      "precision of the model on thevalidationdata: 48.445438385009766%\n",
      "Epoch [251/1000],step[195000] Loss:0.1713\n",
      "precision of the model on thetrainingdata: 96.26556396484375%\n",
      "precision of the model on thevalidationdata: 48.1313362121582%\n",
      "Epoch [253/1000],step[196000] Loss:0.1644\n",
      "precision of the model on thetrainingdata: 92.18106842041016%\n",
      "precision of the model on thevalidationdata: 48.30234146118164%\n",
      "Epoch [254/1000],step[197000] Loss:0.1827\n",
      "precision of the model on thetrainingdata: 92.8853759765625%\n",
      "precision of the model on thevalidationdata: 48.20199966430664%\n",
      "Epoch [255/1000],step[198000] Loss:0.1646\n",
      "precision of the model on thetrainingdata: 96.13733673095703%\n",
      "precision of the model on thevalidationdata: 48.384639739990234%\n",
      "Epoch [257/1000],step[199000] Loss:0.1498\n",
      "precision of the model on thetrainingdata: 93.62549591064453%\n",
      "precision of the model on thevalidationdata: 48.17766571044922%\n",
      "Epoch [258/1000],step[200000] Loss:0.1391\n",
      "precision of the model on thetrainingdata: 95.74468231201172%\n",
      "precision of the model on thevalidationdata: 48.33491897583008%\n",
      "Epoch [259/1000],step[201000] Loss:0.1609\n",
      "precision of the model on thetrainingdata: 93.65079498291016%\n",
      "precision of the model on thevalidationdata: 48.35368728637695%\n",
      "Epoch [260/1000],step[202000] Loss:0.1361\n",
      "precision of the model on thetrainingdata: 96.10389709472656%\n",
      "precision of the model on thevalidationdata: 48.32238006591797%\n",
      "Epoch [262/1000],step[203000] Loss:0.1393\n",
      "precision of the model on thetrainingdata: 96.20252990722656%\n",
      "precision of the model on thevalidationdata: 48.39301681518555%\n",
      "Epoch [263/1000],step[204000] Loss:0.2005\n",
      "precision of the model on thetrainingdata: 94.78260803222656%\n",
      "precision of the model on thevalidationdata: 48.42302322387695%\n",
      "Epoch [264/1000],step[205000] Loss:0.1572\n",
      "precision of the model on thetrainingdata: 95.31914520263672%\n",
      "precision of the model on thevalidationdata: 48.35580062866211%\n",
      "Epoch [266/1000],step[206000] Loss:0.1173\n",
      "precision of the model on thetrainingdata: 97.22222137451172%\n",
      "precision of the model on thevalidationdata: 48.647804260253906%\n",
      "Epoch [267/1000],step[207000] Loss:0.1299\n",
      "precision of the model on thetrainingdata: 97.13114929199219%\n",
      "precision of the model on thevalidationdata: 48.45374298095703%\n",
      "Epoch [268/1000],step[208000] Loss:0.1401\n",
      "precision of the model on thetrainingdata: 94.71365356445312%\n",
      "precision of the model on thevalidationdata: 48.444766998291016%\n",
      "Epoch [269/1000],step[209000] Loss:0.1670\n",
      "precision of the model on thetrainingdata: 93.5622329711914%\n",
      "precision of the model on thevalidationdata: 48.67800521850586%\n",
      "Epoch [271/1000],step[210000] Loss:0.1282\n",
      "precision of the model on thetrainingdata: 97.28506469726562%\n",
      "precision of the model on thevalidationdata: 48.29949188232422%\n",
      "Epoch [272/1000],step[211000] Loss:0.1555\n",
      "precision of the model on thetrainingdata: 96.18643951416016%\n",
      "precision of the model on thevalidationdata: 48.56956481933594%\n",
      "Epoch [273/1000],step[212000] Loss:0.1995\n",
      "precision of the model on thetrainingdata: 94.97908020019531%\n",
      "precision of the model on thevalidationdata: 48.287841796875%\n",
      "Epoch [275/1000],step[213000] Loss:0.1424\n",
      "precision of the model on thetrainingdata: 96.49805450439453%\n",
      "precision of the model on thevalidationdata: 48.32304382324219%\n",
      "Epoch [276/1000],step[214000] Loss:0.1531\n",
      "precision of the model on thetrainingdata: 97.23502349853516%\n",
      "precision of the model on thevalidationdata: 48.29352951049805%\n",
      "Epoch [277/1000],step[215000] Loss:0.1319\n",
      "precision of the model on thetrainingdata: 97.9757080078125%\n",
      "precision of the model on thevalidationdata: 48.452110290527344%\n",
      "Epoch [278/1000],step[216000] Loss:0.1268\n",
      "precision of the model on thetrainingdata: 96.49805450439453%\n",
      "precision of the model on thevalidationdata: 48.299015045166016%\n",
      "Epoch [280/1000],step[217000] Loss:0.1545\n",
      "precision of the model on thetrainingdata: 96.01593780517578%\n",
      "precision of the model on thevalidationdata: 48.345645904541016%\n",
      "Epoch [281/1000],step[218000] Loss:0.1109\n",
      "precision of the model on thetrainingdata: 97.07112884521484%\n",
      "precision of the model on thevalidationdata: 48.294921875%\n",
      "Epoch [282/1000],step[219000] Loss:0.1323\n",
      "precision of the model on thetrainingdata: 97.05882263183594%\n",
      "precision of the model on thevalidationdata: 48.251747131347656%\n",
      "Epoch [284/1000],step[220000] Loss:0.1723\n",
      "precision of the model on thetrainingdata: 94.9579849243164%\n",
      "precision of the model on thevalidationdata: 48.42726135253906%\n",
      "Epoch [285/1000],step[221000] Loss:0.1211\n",
      "precision of the model on thetrainingdata: 98.29059600830078%\n",
      "precision of the model on thevalidationdata: 48.35335922241211%\n",
      "Epoch [286/1000],step[222000] Loss:0.1693\n",
      "precision of the model on thetrainingdata: 95.1219482421875%\n",
      "precision of the model on thevalidationdata: 48.41169738769531%\n",
      "Epoch [288/1000],step[223000] Loss:0.1067\n",
      "precision of the model on thetrainingdata: 97.008544921875%\n",
      "precision of the model on thevalidationdata: 48.27693557739258%\n",
      "Epoch [289/1000],step[224000] Loss:0.1461\n",
      "precision of the model on thetrainingdata: 95.91836547851562%\n",
      "precision of the model on thevalidationdata: 48.20309066772461%\n",
      "Epoch [290/1000],step[225000] Loss:0.1159\n",
      "precision of the model on thetrainingdata: 97.5%\n",
      "precision of the model on thevalidationdata: 48.19341278076172%\n",
      "Epoch [291/1000],step[226000] Loss:0.1098\n",
      "precision of the model on thetrainingdata: 96.23430633544922%\n",
      "precision of the model on thevalidationdata: 48.323577880859375%\n",
      "Epoch [293/1000],step[227000] Loss:0.1218\n",
      "precision of the model on thetrainingdata: 97.76119232177734%\n",
      "precision of the model on thevalidationdata: 48.21720504760742%\n",
      "Epoch [294/1000],step[228000] Loss:0.1593\n",
      "precision of the model on thetrainingdata: 94.65020751953125%\n",
      "precision of the model on thevalidationdata: 48.44318389892578%\n",
      "Epoch [295/1000],step[229000] Loss:0.1240\n",
      "precision of the model on thetrainingdata: 96.41434478759766%\n",
      "precision of the model on thevalidationdata: 48.419944763183594%\n",
      "Epoch [297/1000],step[230000] Loss:0.3790\n",
      "precision of the model on thetrainingdata: 84.82490539550781%\n",
      "precision of the model on thevalidationdata: 48.36189651489258%\n",
      "Epoch [298/1000],step[231000] Loss:0.1100\n",
      "precision of the model on thetrainingdata: 97.17742156982422%\n",
      "precision of the model on thevalidationdata: 48.34851837158203%\n",
      "Epoch [299/1000],step[232000] Loss:0.1573\n",
      "precision of the model on thetrainingdata: 94.9579849243164%\n",
      "precision of the model on thevalidationdata: 48.3536376953125%\n",
      "Epoch [300/1000],step[233000] Loss:0.1870\n",
      "precision of the model on thetrainingdata: 94.50980377197266%\n",
      "precision of the model on thevalidationdata: 48.573707580566406%\n",
      "Epoch [302/1000],step[234000] Loss:0.1747\n",
      "precision of the model on thetrainingdata: 95.04132080078125%\n",
      "precision of the model on thevalidationdata: 48.53240203857422%\n",
      "Epoch [303/1000],step[235000] Loss:0.1418\n",
      "precision of the model on thetrainingdata: 96.31147766113281%\n",
      "precision of the model on thevalidationdata: 48.15779495239258%\n",
      "Epoch [304/1000],step[236000] Loss:0.1490\n",
      "precision of the model on thetrainingdata: 95.703125%\n",
      "precision of the model on thevalidationdata: 48.3493537902832%\n",
      "Epoch [306/1000],step[237000] Loss:0.2856\n",
      "precision of the model on thetrainingdata: 89.1891860961914%\n",
      "precision of the model on thevalidationdata: 48.218055725097656%\n",
      "Epoch [307/1000],step[238000] Loss:0.1010\n",
      "precision of the model on thetrainingdata: 97.93388366699219%\n",
      "precision of the model on thevalidationdata: 48.331565856933594%\n",
      "Epoch [308/1000],step[239000] Loss:0.1162\n",
      "precision of the model on thetrainingdata: 99.08256530761719%\n",
      "precision of the model on thevalidationdata: 48.095863342285156%\n",
      "Epoch [309/1000],step[240000] Loss:0.1075\n",
      "precision of the model on thetrainingdata: 95.21739196777344%\n",
      "precision of the model on thevalidationdata: 48.3801383972168%\n",
      "Epoch [311/1000],step[241000] Loss:0.1317\n",
      "precision of the model on thetrainingdata: 97.14286041259766%\n",
      "precision of the model on thevalidationdata: 48.53358459472656%\n",
      "Epoch [312/1000],step[242000] Loss:0.1187\n",
      "precision of the model on thetrainingdata: 96.20252990722656%\n",
      "precision of the model on thevalidationdata: 48.23767852783203%\n",
      "Epoch [313/1000],step[243000] Loss:0.1109\n",
      "precision of the model on thetrainingdata: 96.6942138671875%\n",
      "precision of the model on thevalidationdata: 48.2407112121582%\n",
      "Epoch [315/1000],step[244000] Loss:0.1672\n",
      "precision of the model on thetrainingdata: 96.70781707763672%\n",
      "precision of the model on thevalidationdata: 48.56637954711914%\n",
      "Epoch [316/1000],step[245000] Loss:0.1024\n",
      "precision of the model on thetrainingdata: 96.35627746582031%\n",
      "precision of the model on thevalidationdata: 48.20517349243164%\n",
      "Epoch [317/1000],step[246000] Loss:0.1280\n",
      "precision of the model on thetrainingdata: 94.90196228027344%\n",
      "precision of the model on thevalidationdata: 48.14216613769531%\n",
      "Epoch [318/1000],step[247000] Loss:0.1421\n",
      "precision of the model on thetrainingdata: 97.95918273925781%\n",
      "precision of the model on thevalidationdata: 48.304115295410156%\n",
      "Epoch [320/1000],step[248000] Loss:0.1317\n",
      "precision of the model on thetrainingdata: 94.17040252685547%\n",
      "precision of the model on thevalidationdata: 48.412288665771484%\n",
      "Epoch [321/1000],step[249000] Loss:0.0891\n",
      "precision of the model on thetrainingdata: 97.5%\n",
      "precision of the model on thevalidationdata: 48.46283721923828%\n",
      "Epoch [322/1000],step[250000] Loss:0.1130\n",
      "precision of the model on thetrainingdata: 98.80952453613281%\n",
      "precision of the model on thevalidationdata: 48.29664611816406%\n",
      "Epoch [324/1000],step[251000] Loss:0.1363\n",
      "precision of the model on thetrainingdata: 96.68049621582031%\n",
      "precision of the model on thevalidationdata: 48.25347900390625%\n",
      "Epoch [325/1000],step[252000] Loss:0.1364\n",
      "precision of the model on thetrainingdata: 98.63636016845703%\n",
      "precision of the model on thevalidationdata: 48.54658889770508%\n",
      "Epoch [326/1000],step[253000] Loss:0.1530\n",
      "precision of the model on thetrainingdata: 97.07112884521484%\n",
      "precision of the model on thevalidationdata: 48.34202194213867%\n",
      "Epoch [327/1000],step[254000] Loss:0.1387\n",
      "precision of the model on thetrainingdata: 98.75518798828125%\n",
      "precision of the model on thevalidationdata: 48.226356506347656%\n",
      "Epoch [329/1000],step[255000] Loss:0.1386\n",
      "precision of the model on thetrainingdata: 95.68965148925781%\n",
      "precision of the model on thevalidationdata: 48.56778335571289%\n",
      "Epoch [330/1000],step[256000] Loss:0.1075\n",
      "precision of the model on thetrainingdata: 97.58064270019531%\n",
      "precision of the model on thevalidationdata: 48.18707275390625%\n",
      "Epoch [331/1000],step[257000] Loss:0.1197\n",
      "precision of the model on thetrainingdata: 97.54098510742188%\n",
      "precision of the model on thevalidationdata: 48.485496520996094%\n",
      "Epoch [333/1000],step[258000] Loss:0.0864\n",
      "precision of the model on thetrainingdata: 97.39130401611328%\n",
      "precision of the model on thevalidationdata: 48.60943603515625%\n",
      "Epoch [334/1000],step[259000] Loss:0.1471\n",
      "precision of the model on thetrainingdata: 95.51020050048828%\n",
      "precision of the model on thevalidationdata: 48.33808135986328%\n",
      "Epoch [335/1000],step[260000] Loss:0.1092\n",
      "precision of the model on thetrainingdata: 96.49805450439453%\n",
      "precision of the model on thevalidationdata: 48.5800895690918%\n",
      "Epoch [336/1000],step[261000] Loss:0.0987\n",
      "precision of the model on thetrainingdata: 99.15254211425781%\n",
      "precision of the model on thevalidationdata: 48.413414001464844%\n",
      "Epoch [338/1000],step[262000] Loss:0.1114\n",
      "precision of the model on thetrainingdata: 96.25%\n",
      "precision of the model on thevalidationdata: 48.32368469238281%\n",
      "Epoch [339/1000],step[263000] Loss:0.1376\n",
      "precision of the model on thetrainingdata: 94.80000305175781%\n",
      "precision of the model on thevalidationdata: 48.04457092285156%\n",
      "Epoch [340/1000],step[264000] Loss:0.1170\n",
      "precision of the model on thetrainingdata: 97.52066040039062%\n",
      "precision of the model on thevalidationdata: 48.2717170715332%\n",
      "Epoch [342/1000],step[265000] Loss:0.1268\n",
      "precision of the model on thetrainingdata: 96.93486785888672%\n",
      "precision of the model on thevalidationdata: 48.348388671875%\n",
      "Epoch [343/1000],step[266000] Loss:0.0783\n",
      "precision of the model on thetrainingdata: 99.5850601196289%\n",
      "precision of the model on thevalidationdata: 48.47751998901367%\n",
      "Epoch [344/1000],step[267000] Loss:0.1022\n",
      "precision of the model on thetrainingdata: 97.79735565185547%\n",
      "precision of the model on thevalidationdata: 48.31742477416992%\n",
      "Epoch [345/1000],step[268000] Loss:0.0820\n",
      "precision of the model on thetrainingdata: 97.39776611328125%\n",
      "precision of the model on thevalidationdata: 48.21745300292969%\n",
      "Epoch [347/1000],step[269000] Loss:0.1332\n",
      "precision of the model on thetrainingdata: 98.79032135009766%\n",
      "precision of the model on thevalidationdata: 48.02854537963867%\n",
      "Epoch [348/1000],step[270000] Loss:0.1235\n",
      "precision of the model on thetrainingdata: 97.95918273925781%\n",
      "precision of the model on thevalidationdata: 48.18138122558594%\n",
      "Epoch [349/1000],step[271000] Loss:0.1142\n",
      "precision of the model on thetrainingdata: 97.11933898925781%\n",
      "precision of the model on thevalidationdata: 48.02088165283203%\n",
      "Epoch [351/1000],step[272000] Loss:0.1391\n",
      "precision of the model on thetrainingdata: 97.08333587646484%\n",
      "precision of the model on thevalidationdata: 48.325923919677734%\n",
      "Epoch [352/1000],step[273000] Loss:0.1028\n",
      "precision of the model on thetrainingdata: 98.26839447021484%\n",
      "precision of the model on thevalidationdata: 48.63266372680664%\n",
      "Epoch [353/1000],step[274000] Loss:0.0757\n",
      "precision of the model on thetrainingdata: 98.37398529052734%\n",
      "precision of the model on thevalidationdata: 48.356502532958984%\n",
      "Epoch [354/1000],step[275000] Loss:0.1066\n",
      "precision of the model on thetrainingdata: 94.97908020019531%\n",
      "precision of the model on thevalidationdata: 48.1708869934082%\n",
      "Epoch [356/1000],step[276000] Loss:0.0983\n",
      "precision of the model on thetrainingdata: 97.75785064697266%\n",
      "precision of the model on thevalidationdata: 48.45262908935547%\n",
      "Epoch [357/1000],step[277000] Loss:0.1820\n",
      "precision of the model on thetrainingdata: 96.38554382324219%\n",
      "precision of the model on thevalidationdata: 48.26751708984375%\n",
      "Epoch [358/1000],step[278000] Loss:0.0833\n",
      "precision of the model on thetrainingdata: 98.2978744506836%\n",
      "precision of the model on thevalidationdata: 48.3322868347168%\n",
      "Epoch [360/1000],step[279000] Loss:0.1345\n",
      "precision of the model on thetrainingdata: 96.0%\n",
      "precision of the model on thevalidationdata: 48.13178253173828%\n",
      "Epoch [361/1000],step[280000] Loss:0.1328\n",
      "precision of the model on thetrainingdata: 95.51020050048828%\n",
      "precision of the model on thevalidationdata: 48.346763610839844%\n",
      "Epoch [362/1000],step[281000] Loss:0.1727\n",
      "precision of the model on thetrainingdata: 96.78714752197266%\n",
      "precision of the model on thevalidationdata: 48.55354690551758%\n",
      "Epoch [363/1000],step[282000] Loss:0.1091\n",
      "precision of the model on thetrainingdata: 97.17742156982422%\n",
      "precision of the model on thevalidationdata: 48.39512634277344%\n",
      "Epoch [365/1000],step[283000] Loss:0.0670\n",
      "precision of the model on thetrainingdata: 99.19999694824219%\n",
      "precision of the model on thevalidationdata: 48.27601623535156%\n",
      "Epoch [366/1000],step[284000] Loss:0.0907\n",
      "precision of the model on thetrainingdata: 99.13420104980469%\n",
      "precision of the model on thevalidationdata: 48.30990982055664%\n",
      "Epoch [367/1000],step[285000] Loss:0.0739\n",
      "precision of the model on thetrainingdata: 99.14530181884766%\n",
      "precision of the model on thevalidationdata: 48.325504302978516%\n",
      "Epoch [369/1000],step[286000] Loss:0.1332\n",
      "precision of the model on thetrainingdata: 96.61016845703125%\n",
      "precision of the model on thevalidationdata: 48.37449645996094%\n",
      "Epoch [370/1000],step[287000] Loss:0.1788\n",
      "precision of the model on thetrainingdata: 93.3054428100586%\n",
      "precision of the model on thevalidationdata: 48.31620407104492%\n",
      "Epoch [371/1000],step[288000] Loss:0.1092\n",
      "precision of the model on thetrainingdata: 98.046875%\n",
      "precision of the model on thevalidationdata: 48.44709777832031%\n",
      "Epoch [372/1000],step[289000] Loss:0.0667\n",
      "precision of the model on thetrainingdata: 98.31932830810547%\n",
      "precision of the model on thevalidationdata: 48.37472915649414%\n",
      "Epoch [374/1000],step[290000] Loss:0.0953\n",
      "precision of the model on thetrainingdata: 96.38008880615234%\n",
      "precision of the model on thevalidationdata: 48.1518440246582%\n",
      "Epoch [375/1000],step[291000] Loss:0.1437\n",
      "precision of the model on thetrainingdata: 96.04743194580078%\n",
      "precision of the model on thevalidationdata: 48.019691467285156%\n",
      "Epoch [376/1000],step[292000] Loss:0.1004\n",
      "precision of the model on thetrainingdata: 98.2062759399414%\n",
      "precision of the model on thevalidationdata: 48.57723617553711%\n",
      "Epoch [378/1000],step[293000] Loss:0.1293\n",
      "precision of the model on thetrainingdata: 95.31914520263672%\n",
      "precision of the model on thevalidationdata: 48.04098892211914%\n",
      "Epoch [379/1000],step[294000] Loss:0.0980\n",
      "precision of the model on thetrainingdata: 97.03389739990234%\n",
      "precision of the model on thevalidationdata: 48.540618896484375%\n",
      "Epoch [380/1000],step[295000] Loss:0.1082\n",
      "precision of the model on thetrainingdata: 98.30508422851562%\n",
      "precision of the model on thevalidationdata: 48.42336654663086%\n",
      "Epoch [381/1000],step[296000] Loss:0.1079\n",
      "precision of the model on thetrainingdata: 96.66666412353516%\n",
      "precision of the model on thevalidationdata: 48.16401290893555%\n",
      "Epoch [383/1000],step[297000] Loss:0.0985\n",
      "precision of the model on thetrainingdata: 97.94239044189453%\n",
      "precision of the model on thevalidationdata: 48.27214813232422%\n",
      "Epoch [384/1000],step[298000] Loss:0.0855\n",
      "precision of the model on thetrainingdata: 97.85408020019531%\n",
      "precision of the model on thevalidationdata: 48.44733810424805%\n",
      "Epoch [385/1000],step[299000] Loss:0.0884\n",
      "precision of the model on thetrainingdata: 97.17742156982422%\n",
      "precision of the model on thevalidationdata: 48.3739013671875%\n",
      "Epoch [387/1000],step[300000] Loss:0.0592\n",
      "precision of the model on thetrainingdata: 98.77049255371094%\n",
      "precision of the model on thevalidationdata: 48.489341735839844%\n",
      "Epoch [388/1000],step[301000] Loss:0.0809\n",
      "precision of the model on thetrainingdata: 97.98387145996094%\n",
      "precision of the model on thevalidationdata: 48.4852409362793%\n",
      "Epoch [389/1000],step[302000] Loss:0.1509\n",
      "precision of the model on thetrainingdata: 95.0450439453125%\n",
      "precision of the model on thevalidationdata: 48.208255767822266%\n",
      "Epoch [390/1000],step[303000] Loss:0.0794\n",
      "precision of the model on thetrainingdata: 97.95918273925781%\n",
      "precision of the model on thevalidationdata: 48.18454360961914%\n",
      "Epoch [392/1000],step[304000] Loss:0.0666\n",
      "precision of the model on thetrainingdata: 98.4126968383789%\n",
      "precision of the model on thevalidationdata: 48.485992431640625%\n",
      "Epoch [393/1000],step[305000] Loss:0.1181\n",
      "precision of the model on thetrainingdata: 98.80952453613281%\n",
      "precision of the model on thevalidationdata: 48.44342041015625%\n",
      "Epoch [394/1000],step[306000] Loss:0.2687\n",
      "precision of the model on thetrainingdata: 91.94915008544922%\n",
      "precision of the model on thevalidationdata: 48.472652435302734%\n",
      "Epoch [396/1000],step[307000] Loss:0.1539\n",
      "precision of the model on thetrainingdata: 96.55172729492188%\n",
      "precision of the model on thevalidationdata: 48.01534652709961%\n",
      "Epoch [397/1000],step[308000] Loss:0.0910\n",
      "precision of the model on thetrainingdata: 98.80477905273438%\n",
      "precision of the model on thevalidationdata: 48.329978942871094%\n",
      "Epoch [398/1000],step[309000] Loss:0.1110\n",
      "precision of the model on thetrainingdata: 97.08333587646484%\n",
      "precision of the model on thevalidationdata: 48.31173324584961%\n",
      "Epoch [399/1000],step[310000] Loss:0.0783\n",
      "precision of the model on thetrainingdata: 98.7854232788086%\n",
      "precision of the model on thevalidationdata: 48.3486442565918%\n",
      "Epoch [401/1000],step[311000] Loss:0.1088\n",
      "precision of the model on thetrainingdata: 96.55172729492188%\n",
      "precision of the model on thevalidationdata: 48.47487258911133%\n",
      "Epoch [402/1000],step[312000] Loss:0.1204\n",
      "precision of the model on thetrainingdata: 97.18875122070312%\n",
      "precision of the model on thevalidationdata: 48.4787712097168%\n",
      "Epoch [403/1000],step[313000] Loss:0.0512\n",
      "precision of the model on thetrainingdata: 100.0%\n",
      "precision of the model on thevalidationdata: 48.33875274658203%\n",
      "Epoch [405/1000],step[314000] Loss:0.1085\n",
      "precision of the model on thetrainingdata: 97.57084655761719%\n",
      "precision of the model on thevalidationdata: 48.09604263305664%\n",
      "Epoch [406/1000],step[315000] Loss:0.2124\n",
      "precision of the model on thetrainingdata: 92.97520446777344%\n",
      "precision of the model on thevalidationdata: 47.89215850830078%\n",
      "Epoch [407/1000],step[316000] Loss:0.1034\n",
      "precision of the model on thetrainingdata: 98.4313735961914%\n",
      "precision of the model on thevalidationdata: 48.4410514831543%\n",
      "Epoch [408/1000],step[317000] Loss:0.0966\n",
      "precision of the model on thetrainingdata: 97.96747589111328%\n",
      "precision of the model on thevalidationdata: 48.24314880371094%\n",
      "Epoch [410/1000],step[318000] Loss:0.0903\n",
      "precision of the model on thetrainingdata: 96.86274719238281%\n",
      "precision of the model on thevalidationdata: 48.35744094848633%\n",
      "Epoch [411/1000],step[319000] Loss:0.0780\n",
      "precision of the model on thetrainingdata: 98.80000305175781%\n",
      "precision of the model on thevalidationdata: 48.1990966796875%\n",
      "Epoch [412/1000],step[320000] Loss:0.1504\n",
      "precision of the model on thetrainingdata: 98.36065673828125%\n",
      "precision of the model on thevalidationdata: 48.12381362915039%\n",
      "Epoch [414/1000],step[321000] Loss:0.0882\n",
      "precision of the model on thetrainingdata: 97.16981506347656%\n",
      "precision of the model on thevalidationdata: 48.179039001464844%\n",
      "Epoch [415/1000],step[322000] Loss:0.1437\n",
      "precision of the model on thetrainingdata: 94.39655303955078%\n",
      "precision of the model on thevalidationdata: 48.4741325378418%\n",
      "Epoch [416/1000],step[323000] Loss:0.1366\n",
      "precision of the model on thetrainingdata: 96.62447357177734%\n",
      "precision of the model on thevalidationdata: 48.03782653808594%\n",
      "Epoch [417/1000],step[324000] Loss:0.1340\n",
      "precision of the model on thetrainingdata: 97.65625%\n",
      "precision of the model on thevalidationdata: 48.246456146240234%\n",
      "Epoch [419/1000],step[325000] Loss:0.0642\n",
      "precision of the model on thetrainingdata: 98.66071319580078%\n",
      "precision of the model on thevalidationdata: 48.21531677246094%\n",
      "Epoch [420/1000],step[326000] Loss:0.0598\n",
      "precision of the model on thetrainingdata: 99.11894226074219%\n",
      "precision of the model on thevalidationdata: 48.231082916259766%\n",
      "Epoch [421/1000],step[327000] Loss:0.1510\n",
      "precision of the model on thetrainingdata: 97.33333587646484%\n",
      "precision of the model on thevalidationdata: 48.5498046875%\n",
      "Epoch [423/1000],step[328000] Loss:0.1425\n",
      "precision of the model on thetrainingdata: 96.25%\n",
      "precision of the model on thevalidationdata: 48.29802322387695%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7f5970512c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresnet_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-338aa7c4d2bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#残差网络\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-338aa7c4d2bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练\n",
    "total_step=0\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        images, labels = data\n",
    "        images=images.type(torch.FloatTensor)\n",
    "        labels=labels.type(torch.FloatTensor)\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        #forward pass\n",
    "        outputs=resnet_lstm(images)\n",
    "        loss=loss_function(outputs,labels.long())\n",
    "        \n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step+=1\n",
    "        if (total_step)%1000==0:\n",
    "            print(\"Epoch [{}/{}],step[{}] Loss:{:.4f}\".format(epoch+1,num_epochs,total_step,loss.item()))\n",
    "            _,train_pre=predict_precision(resnet_lstm,images,labels,device,predict_type='training')\n",
    "            train_precision.append(train_pre)\n",
    "            for images, labels in valid_loader:\n",
    "                _,valid_pre=predict_precision(resnet_lstm,images,labels,device,predict_type='validation')\n",
    "            valid_precision.append(valid_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'precision')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pic5\n",
    "plt.plot(train_precision,label=\"training precision\")\n",
    "plt.plot(valid_precision,label=\"validation precision\")\n",
    "plt.title(\"resnet_lstm on stock\")\n",
    "plt.xlabel(\"1000*x training step\")\n",
    "plt.ylabel(\"precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trial 1\n",
    "\n",
    "\n",
    "$Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "feature_channel=42\n",
    "hidden_size = 512  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "num_classes = 2\n",
    "batch_size = 1024\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "device = torch.device(\"cuda:1\")$\n",
    "\n",
    "由于整体层数比较多，参数也多，导致可能出现了梯度消失的情况（pic5）。\n",
    "\n",
    "下一步将层数、参数、batch_size缩减做进一步尝试:\n",
    "\n",
    "trial2\n",
    "\n",
    "$Hyper Parameters\n",
    "sequence_length = 8  # 序列长度，将图像的每一列作为一个序列\n",
    "feature_channel=42\n",
    "hidden_size = 128  # 隐藏层的size\n",
    "num_layers =  2 # 有多少层\n",
    "num_classes = 2\n",
    "batch_size = 512\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:1\")\n",
    "激活函数换为leakyrelu$\n",
    "\n",
    "解决了梯度消失的问题，但是预测还是无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
